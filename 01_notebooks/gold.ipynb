{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMFrPtlHxn/1w9oMI2tL2r1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MONTAR O DRIVE"
   ],
   "metadata": {
    "id": "pbQFiFKtkG_y"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P8xAPbDtj5uc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758736056961,
     "user_tz": 180,
     "elapsed": 2668,
     "user": {
      "displayName": "Wilson R. Melo (consultor)",
      "userId": "00911396335732968034"
     }
    },
    "outputId": "e062ebd9-ca39-4c80-c6b4-e43eb75fdfe3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# MONTAR O DRIVE\n",
    "\n",
    "# Importar utilitário de montagem de drive (funciona no Colab e localmente)\n",
    "from importlib.util import spec_from_file_location, module_from_spec\n",
    "import os\n",
    "\n",
    "# Localizar o arquivo utils_drive.py\n",
    "utils_drive_path = os.path.join(os.getcwd(), '02_src', 'utils_drive.py')\n",
    "if not os.path.exists(utils_drive_path):\n",
    "    # Tentar caminho alternativo se estiver em subdiretório\n",
    "    utils_drive_path = os.path.join(os.path.dirname(os.getcwd()), '02_src', 'utils_drive.py')\n",
    "\n",
    "# Importar o módulo dinamicamente\n",
    "spec = spec_from_file_location('utils_drive', utils_drive_path)\n",
    "utils_drive = module_from_spec(spec)\n",
    "spec.loader.exec_module(utils_drive)\n",
    "\n",
    "# Configurar ambiente (monta drive e retorna caminhos)\n",
    "drive_path, project_root = utils_drive.setup_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GERAÇÃO DA BASE"
   ],
   "metadata": {
    "id": "CSrxGC0Ew6LV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VALIDAÇÃO"
   ],
   "metadata": {
    "id": "nPku72g_r09J"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# TROCA DE ESTRATEGISTA - RETORNO AO SILVER PARA CORREÇÃO"
   ],
   "metadata": {
    "id": "QVXOddTX_GZn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# INSTRUÇÃO #0002 — SILVER_REPAIR_AND_HARDEN_V2 (DRY RUN)\n",
    "# Objetivo: Regenerar em RAM o silver_close_20250923_v2.parquet a partir do Bronze\n",
    "#           e adicionar coluna sha256 ao manifesto (em RAM, sem persistir).\n",
    "# Método de calendário: união explícita dos índices dos 24 tickers .SA (Bronze)\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================== CONFIG ===============================\n",
    "DRY_RUN = True  # obrigatório\n",
    "EXPECTED_ROWS = 3409\n",
    "EXPECTED_COLS = 31\n",
    "EXPECTED_DATE_MIN = \"2012-01-02\"\n",
    "EXPECTED_DATE_MAX = \"2025-09-19\"\n",
    "EXPECTED_ARTIFACT_NAME = \"silver_close_20250923_v2.parquet\"\n",
    "\n",
    "MANIFEST_CSV = \"/content/drive/Shareddrives/BOLSA_2026/a_bolsa2026_gemini/00_data/02_silver/silver_manifest_20250923_v2.csv\"\n",
    "\n",
    "# Derivar ROOT e RAW_DIR a partir do caminho do manifesto (sem adivinhar nomes externos).\n",
    "# Resultado esperado: /content/drive/Shareddrives/BOLSA_2026/a_bolsa2026_gemini/00_data/01_raw/\n",
    "proj_root = os.path.dirname(os.path.dirname(MANIFEST_CSV))  # .../a_bolsa2026_gemini/00_data\n",
    "RAW_DIR = os.path.join(proj_root, \"01_raw\")\n",
    "\n",
    "# =========================== GUARD-CLAUSES ============================\n",
    "def fail(msg, payload=None):\n",
    "    print(\"\\n=== VALIDATION_ERROR ===\")\n",
    "    print(msg)\n",
    "    if payload is not None:\n",
    "        try:\n",
    "            print(json.dumps(payload, indent=2, ensure_ascii=False, default=str))\n",
    "        except Exception:\n",
    "            print(payload)\n",
    "    sys.exit(1)\n",
    "\n",
    "if not os.path.exists(MANIFEST_CSV):\n",
    "    fail(\"Manifesto não encontrado no SSOT.\", {\"MANIFEST_CSV\": MANIFEST_CSV})\n",
    "\n",
    "if not os.path.isdir(RAW_DIR):\n",
    "    fail(\"Diretório RAW ausente no SSOT.\", {\"RAW_DIR\": RAW_DIR})\n",
    "\n",
    "# ============================== UTIL ==============================\n",
    "def read_parquet_close_series(fpath):\n",
    "    \"\"\"Lê parquet, detecta coluna Close (case-insensitive) e retorna Series com DatetimeIndex canônico.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_parquet(fpath)\n",
    "    except Exception as e:\n",
    "        fail(\"Falha ao ler Parquet do Bronze.\", {\"arquivo\": fpath, \"erro\": repr(e)})\n",
    "\n",
    "    # Detectar coluna de data (preferência: column; fallback: index)\n",
    "    date_col = None\n",
    "    for cand in (\"date\", \"Date\", \"DATE\"):\n",
    "        if cand in df.columns:\n",
    "            date_col = cand\n",
    "            break\n",
    "    if date_col is None:\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            df = df.copy()\n",
    "            df[\"__date__\"] = df.index\n",
    "            date_col = \"__date__\"\n",
    "        else:\n",
    "            fail(\"Nenhum índice/coluna de data detectado no Bronze.\", {\"arquivo\": fpath, \"index_type\": type(df.index).__name__})\n",
    "\n",
    "    # Detectar coluna Close (case-insensitive)\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    close_col = cols_lower.get(\"close\")\n",
    "    if close_col is None:\n",
    "        fail(\"Coluna 'Close' ausente no artefato do Bronze.\", {\"arquivo\": fpath, \"colunas\": list(df.columns)})\n",
    "\n",
    "    # Normalizar datas\n",
    "    s = df[[date_col, close_col]].copy()\n",
    "    s[date_col] = pd.to_datetime(s[date_col], utc=False, errors=\"coerce\")\n",
    "    if s[date_col].isna().any():\n",
    "        fail(\"Datas inválidas encontradas no Bronze.\", {\"arquivo\": fpath, \"n_nulos\": int(s[date_col].isna().sum())})\n",
    "    s = s.set_index(date_col).sort_index()\n",
    "    if isinstance(s.index, pd.DatetimeIndex) and s.index.tz is not None:\n",
    "        s.index = s.index.tz_localize(None)\n",
    "\n",
    "    # Nome da coluna derivado do arquivo (sem extensão), minúsculo\n",
    "    col_name = os.path.splitext(os.path.basename(fpath))[0].lower()\n",
    "    return col_name, s[close_col].rename(col_name)\n",
    "\n",
    "# ============================== PASSO 1 ===============================\n",
    "# Listar todos os arquivos parquet no RAW\n",
    "parquet_files = sorted([os.path.join(RAW_DIR, f) for f in os.listdir(RAW_DIR) if f.lower().endswith(\".parquet\")])\n",
    "if len(parquet_files) == 0:\n",
    "    fail(\"Nenhum arquivo .parquet encontrado no Bronze.\", {\"RAW_DIR\": RAW_DIR})\n",
    "\n",
    "# Separar 24 tickers (.SA) para calendário e 31 (todos) para tabelão\n",
    "# Critério: nome do arquivo (sem extensão) termina com '_sa' OU '.sa' (case-insensitive)\n",
    "ticker_pat = re.compile(r\"(_sa|\\.sa)$\", re.IGNORECASE)\n",
    "\n",
    "ticker_files = [p for p in parquet_files if ticker_pat.search(os.path.splitext(os.path.basename(p))[0]) is not None]\n",
    "other_files  = [p for p in parquet_files if p not in ticker_files]\n",
    "\n",
    "# Precisamos de exatamente 24 tickers .SA\n",
    "if len(ticker_files) != 24:\n",
    "    fail(\"Quantidade de tickers .SA divergente para calendário.\",\n",
    "         {\"esperado\": 24, \"encontrado\": len(ticker_files),\n",
    "          \"amostra\": [os.path.basename(x) for x in ticker_files][:10]})\n",
    "\n",
    "# Precisamos de exatamente 31 artefatos no total (24 + 7)\n",
    "if len(parquet_files) != 31:\n",
    "    fail(\"Quantidade total de artefatos no Bronze divergente.\",\n",
    "         {\"esperado\": 31, \"encontrado\": len(parquet_files)})\n",
    "\n",
    "# ============================== PASSO 2 ===============================\n",
    "# Carregar os 24 tickers para construir o calendário (união dos índices)\n",
    "calendar_indices = []\n",
    "for fpath in ticker_files:\n",
    "    _, ser = read_parquet_close_series(fpath)\n",
    "    if not isinstance(ser.index, pd.DatetimeIndex):\n",
    "        fail(\"Série sem DatetimeIndex ao construir calendário.\", {\"arquivo\": fpath})\n",
    "    calendar_indices.append(ser.index)\n",
    "\n",
    "# União dos índices, ordenada, única e tz-free\n",
    "if len(calendar_indices) == 0:\n",
    "    fail(\"Nenhum índice coletado para calendário.\", {})\n",
    "\n",
    "cal_union = calendar_indices[0]\n",
    "for idx in calendar_indices[1:]:\n",
    "    cal_union = cal_union.union(idx)\n",
    "\n",
    "# Normalizar resultados\n",
    "cal_union = pd.DatetimeIndex(pd.to_datetime(cal_union, utc=False)).tz_localize(None).sort_values().unique()\n",
    "\n",
    "# Validar contagem do calendário\n",
    "if len(cal_union) != EXPECTED_ROWS:\n",
    "    fail(\"contagem_pregoes_calendario_invalida\",\n",
    "         {\"esperado\": EXPECTED_ROWS, \"obtido\": int(len(cal_union))})\n",
    "\n",
    "# Validar faixa de datas\n",
    "if (cal_union.min() != pd.Timestamp(EXPECTED_DATE_MIN)) or (cal_union.max() != pd.Timestamp(EXPECTED_DATE_MAX)):\n",
    "    fail(\"faixa_de_datas_calendario_invalida\",\n",
    "         {\"esperado_min\": EXPECTED_DATE_MIN, \"obtido_min\": str(cal_union.min().date()),\n",
    "          \"esperado_max\": EXPECTED_DATE_MAX, \"obtido_max\": str(cal_union.max().date())})\n",
    "\n",
    "# ============================== PASSO 3 ===============================\n",
    "# Carregar TODOS os 31 ativos (24 tickers + 7 macros) e montar o tabelão por outer join no calendário\n",
    "series_map = {}\n",
    "for fpath in parquet_files:\n",
    "    name, ser = read_parquet_close_series(fpath)\n",
    "    if name in series_map:\n",
    "        fail(\"Colisão de nomes de colunas derivadas de arquivos.\", {\"arquivo\": fpath, \"coluna\": name})\n",
    "    # Reindexar no calendário canônico (união), garantindo outer join controlado\n",
    "    ser2 = ser.reindex(cal_union)\n",
    "    series_map[name] = ser2\n",
    "\n",
    "# Build do DataFrame final\n",
    "df_silver_close = pd.DataFrame(index=cal_union, data={k: v for k, v in series_map.items()})\n",
    "\n",
    "# ============================== PASSO 4 ===============================\n",
    "# Validar estrutura em RAM: shape, datas, cardinalidade\n",
    "shape_ok = (df_silver_close.shape == (EXPECTED_ROWS, EXPECTED_COLS))\n",
    "if not shape_ok:\n",
    "    fail(\"shape_regenerado_invalido\",\n",
    "         {\"esperado\": (EXPECTED_ROWS, EXPECTED_COLS), \"obtido\": df_silver_close.shape})\n",
    "\n",
    "idx = df_silver_close.index\n",
    "if (idx.min() != pd.Timestamp(EXPECTED_DATE_MIN)) or (idx.max() != pd.Timestamp(EXPECTED_DATE_MAX)):\n",
    "    fail(\"faixa_de_datas_invalida\",\n",
    "         {\"esperado_min\": EXPECTED_DATE_MIN, \"obtido_min\": str(idx.min().date()),\n",
    "          \"esperado_max\": EXPECTED_DATE_MAX, \"obtido_max\": str(idx.max().date())})\n",
    "\n",
    "# ============================== PASSO 5 ===============================\n",
    "# Calcular hash sha256 simulando persistência (buffer de bytes em memória)\n",
    "buff = io.BytesIO()\n",
    "try:\n",
    "    df_silver_close.to_parquet(buff, index=True)\n",
    "except Exception as e:\n",
    "    fail(\"Falha ao serializar df_silver_close para Parquet em memória.\", {\"erro\": repr(e)})\n",
    "buff.seek(0)\n",
    "sha256_hex = hashlib.sha256(buff.read()).hexdigest()\n",
    "\n",
    "# ============================== PASSO 6 ===============================\n",
    "# Atualizar manifesto em RAM: carregar CSV, localizar linha do ARTIFACT_NAME, inserir/atualizar 'sha256'\n",
    "try:\n",
    "    df_manifest = pd.read_csv(MANIFEST_CSV)\n",
    "except Exception as e:\n",
    "    fail(\"Falha ao ler o manifesto Silver.\", {\"MANIFEST_CSV\": MANIFEST_CSV, \"erro\": repr(e)})\n",
    "\n",
    "if \"artifact_name\" not in df_manifest.columns:\n",
    "    fail(\"Manifesto inválido: coluna 'artifact_name' ausente.\", {\"colunas\": list(df_manifest.columns)})\n",
    "\n",
    "rows_match = df_manifest[\"artifact_name\"] == EXPECTED_ARTIFACT_NAME\n",
    "n_matches = int(rows_match.sum())\n",
    "if n_matches != 1:\n",
    "    fail(\"Manifesto inválido: linha do ARTIFACT_NAME não é única.\",\n",
    "         {\"artifact_name\": EXPECTED_ARTIFACT_NAME, \"linhas_encontradas\": n_matches})\n",
    "\n",
    "# Inserir/atualizar coluna sha256 em RAM\n",
    "df_manifest = df_manifest.copy()\n",
    "if \"sha256\" not in df_manifest.columns:\n",
    "    df_manifest[\"sha256\"] = None\n",
    "df_manifest.loc[rows_match, \"sha256\"] = sha256_hex\n",
    "\n",
    "# ============================== PASSO 7 ===============================\n",
    "# RELATÓRIO (DRY RUN) — nenhuma escrita em disco\n",
    "print(\"\\n====================== DRY RUN — RELATÓRIO ======================\")\n",
    "print(\"Tickers (.SA) carregados para calendário:\", len(ticker_files), \"(esperado=24)\")\n",
    "print(\"Calendário canônico (união de índices):\", f\"{idx.min().date()} → {idx.max().date()} (linhas={len(idx)})\")\n",
    "print(\"df_silver_close.shape:\", df_silver_close.shape, \" | esperado:\", (EXPECTED_ROWS, EXPECTED_COLS))\n",
    "\n",
    "print(\"\\n--- df_silver_close.head() ---\")\n",
    "print(df_silver_close.head())\n",
    "\n",
    "print(\"\\n--- df_silver_close.info() ---\")\n",
    "buf_info = io.StringIO()\n",
    "df_silver_close.info(buf=buf_info)\n",
    "print(buf_info.getvalue())\n",
    "\n",
    "print(\"\\nsha256 do artefato regenerado (Parquet em memória):\")\n",
    "print(sha256_hex)\n",
    "\n",
    "print(\"\\n--- Linha do manifesto atualizada (em RAM) ---\")\n",
    "print(df_manifest.loc[rows_match])\n",
    "\n",
    "# ============================== PASSO 8 ===============================\n",
    "# CHECKLIST\n",
    "checklist = {\n",
    "    \"24 tickers .SA carregados p/ calendário\": len(ticker_files) == 24,\n",
    "    \"Calendário união == 3409 dias\": len(idx) == EXPECTED_ROWS,\n",
    "    \"df_silver_close shape (3409,31)\": df_silver_close.shape == (EXPECTED_ROWS, EXPECTED_COLS),\n",
    "    \"sha256 calculado exibido\": isinstance(sha256_hex, str) and len(sha256_hex) == 64,\n",
    "    \"Manifesto carregado e linha atualizada em RAM\": n_matches == 1 and df_manifest.loc[rows_match, \"sha256\"].iloc[0] == sha256_hex,\n",
    "    \"Nenhuma escrita em disco (DRY RUN)\": DRY_RUN is True,\n",
    "    \"Relatório emitido\": True,\n",
    "}\n",
    "print(\"\\n--- CHECKLIST ---\")\n",
    "print(json.dumps(checklist, indent=2, ensure_ascii=False))\n",
    "\n",
    "print(\"\\nSTATUS_FINAL: OK (DRY RUN) — Nada foi persistido.\")\n"
   ],
   "metadata": {
    "id": "eesAQ3jaA2Jq",
    "executionInfo": {
     "status": "error",
     "timestamp": 1758739002575,
     "user_tz": 180,
     "elapsed": 79,
     "user": {
      "displayName": "Wilson R. Melo (consultor)",
      "userId": "00911396335732968034"
     }
    },
    "outputId": "5af3b82b-bd54-48b7-9d2f-807f8a3859b8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    }
   },
   "execution_count": 42,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=== VALIDATION_ERROR ===\n",
      "Quantidade de tickers .SA divergente para calendário.\n",
      "{\n",
      "  \"esperado\": 24,\n",
      "  \"encontrado\": 0,\n",
      "  \"amostra\": []\n",
      "}\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "SystemExit",
     "evalue": "1",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ]
  }
 ]
}