{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montar ou identificar os diretórios locais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Google Drive detectado localmente em: G:\\\\Drives compartilhados\n",
      "IN_COLAB = False\n",
      "DRIVE_ROOT = G:\\\\Drives compartilhados\n",
      "SHARED_DRIVES = G:\\Drives compartilhados\n"
     ]
    }
   ],
   "source": [
    "# Montar Google Drive (Colab) ou detectar Google Drive Desktop (local)\n",
    "# - Em Colab: monta em /content/drive\n",
    "# - Local (Windows): usa Google Drive for Desktop (ex.: G:\\Drives compartilhados)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    from google.colab import drive as _gdrive  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "DRIVE_ROOT = None  # caminho raiz do Drive\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Monta o Google Drive no Colab\n",
    "    _gdrive.mount('/content/drive', force_remount=False)\n",
    "    DRIVE_ROOT = '/content/drive'\n",
    "    print(f\"[OK] Google Drive montado no Colab em: {DRIVE_ROOT}\")\n",
    "else:\n",
    "    # Detecta Google Drive for Desktop no Windows\n",
    "    candidates = [\n",
    "        r'G:\\\\Drives compartilhados',   # PT-BR: Shared drives\n",
    "        r'G:\\\\Shared drives',           # EN: Shared drives\n",
    "        r'G:\\\\My Drive',                # EN: Meu Drive (conta pessoal)\n",
    "        os.path.expandvars(r'%USERPROFILE%\\\\Google Drive'),  # legado\n",
    "        os.path.expandvars(r'%USERPROFILE%\\\\Google Drive (Shared drives)'),\n",
    "    ]\n",
    "    DRIVE_ROOT = next((p for p in candidates if os.path.exists(p)), None)\n",
    "    if DRIVE_ROOT:\n",
    "        print(f\"[OK] Google Drive detectado localmente em: {DRIVE_ROOT}\")\n",
    "    else:\n",
    "        print('[AVISO] Google Drive não encontrado automaticamente.\\n'\n",
    "              '       Verifique se o Google Drive for Desktop está instalado e o caminho correto (ex.: G:\\\\Drives compartilhados).')\n",
    "\n",
    "# Variáveis úteis para uso posterior\n",
    "SHARED_DRIVES = str(Path(DRIVE_ROOT) if DRIVE_ROOT else '')\n",
    "print('IN_COLAB =', IN_COLAB)\n",
    "print('DRIVE_ROOT =', DRIVE_ROOT)\n",
    "print('SHARED_DRIVES =', SHARED_DRIVES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrados 31 arquivos parquet em G:\\Drives compartilhados\\BOLSA_2026\\a_bolsa2026_gemini\\00_data\\01_raw\n",
      "\n",
      "Erros de leitura:\n",
      "Nenhum erro\n",
      "\n",
      "Colunas comuns (presentes em todos):\n",
      "['Adj Close', 'Close', 'Date', 'Dividends', 'High', 'Low', 'Open', 'Stock Splits', 'Volume']\n",
      "\n",
      "Tipos por coluna (top-1):\n",
      "- Adj Close: double (freq 31/31)\n",
      "- Capital Gains: double (freq 1/31)\n",
      "- Close: double (freq 31/31)\n",
      "- Date: timestamp[ns, tz=America/Sao_Paulo] (freq 25/31)\n",
      "- Dividends: double (freq 31/31)\n",
      "- High: double (freq 31/31)\n",
      "- Low: double (freq 31/31)\n",
      "- Open: double (freq 31/31)\n",
      "- Stock Splits: double (freq 31/31)\n",
      "- Volume: int64 (freq 31/31)\n",
      "\n",
      "Amostras de valores (primeiras 5 linhas) para 2 arquivos:\n",
      "\n",
      "== _bvsp_ohlcv_actions_20120101_20250922.parquet (linhas=3400) ==\n",
      "  Open: [57836.0, 59263.0, 59354.0, 58565.0, 58601.0]\n",
      "  High: [59288.0, 59519.0, 59354.0, 59261.0, 59220.0]\n",
      "  Low: [57836.0, 58558.0, 57963.0, 58355.0, 58599.0]\n",
      "  Close: [59265.0, 59365.0, 58546.0, 58600.0, 59083.0]\n",
      "  Adj Close: [59265.0, 59365.0, 58546.0, 58600.0, 59083.0]\n",
      "  Volume: [3083000, 2252000, 2351200, 1659200, 2244600]\n",
      "  Dividends: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Stock Splits: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Date: [Timestamp('2012-01-03 00:00:00-0200', tz='America/Sao_Paulo'), Timestamp('2012-01-04 00:00:00-0200', tz='America/Sao_Paulo'), Timestamp('2012-01-05 00:00:00-0200', tz='America/Sao_Paulo'), Timestamp('2012-01-06 00:00:00-0200', tz='America/Sao_Paulo'), Timestamp('2012-01-09 00:00:00-0200', tz='America/Sao_Paulo')]\n",
      "\n",
      "== _gspc_ohlcv_actions_20120101_20250922.parquet (linhas=3449) ==\n",
      "  Open: [1258.8599853515625, 1277.030029296875, 1277.300048828125, 1280.9300537109375, 1277.8299560546875]\n",
      "  High: [1284.6199951171875, 1278.72998046875, 1283.050048828125, 1281.8399658203125, 1281.989990234375]\n",
      "  Low: [1258.8599853515625, 1268.0999755859375, 1265.260009765625, 1273.3399658203125, 1274.550048828125]\n",
      "  Close: [1277.06005859375, 1277.300048828125, 1281.06005859375, 1277.81005859375, 1280.699951171875]\n",
      "  Adj Close: [1277.06005859375, 1277.300048828125, 1281.06005859375, 1277.81005859375, 1280.699951171875]\n",
      "  Volume: [3943710000, 3592580000, 4315950000, 3656830000, 3371600000]\n",
      "  Dividends: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Stock Splits: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "  Date: [Timestamp('2012-01-03 00:00:00-0500', tz='America/New_York'), Timestamp('2012-01-04 00:00:00-0500', tz='America/New_York'), Timestamp('2012-01-05 00:00:00-0500', tz='America/New_York'), Timestamp('2012-01-06 00:00:00-0500', tz='America/New_York'), Timestamp('2012-01-09 00:00:00-0500', tz='America/New_York')]\n"
     ]
    }
   ],
   "source": [
    "# Verificar parquet em 00_data/01_raw e resumir esquema\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "RAW_DIR = Path('G:/Drives compartilhados/BOLSA_2026/a_bolsa2026_gemini/00_data/01_raw')\n",
    "assert RAW_DIR.exists(), f\"Diretório não encontrado: {RAW_DIR}\"\n",
    "\n",
    "# Garantir pyarrow para leitura robusta\n",
    "try:\n",
    "    import pyarrow.parquet as pq\n",
    "    import pyarrow as pa\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'pyarrow'])\n",
    "    import pyarrow.parquet as pq\n",
    "    import pyarrow as pa\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "files = sorted([p for p in RAW_DIR.glob('*.parquet')])\n",
    "print(f\"Encontrados {len(files)} arquivos parquet em {RAW_DIR}\")\n",
    "\n",
    "schemas = {}\n",
    "errors = {}\n",
    "row_counts = {}\n",
    "example_values = defaultdict(dict)\n",
    "\n",
    "for p in files:\n",
    "    try:\n",
    "        meta = pq.ParquetFile(p)\n",
    "        schema = meta.schema_arrow\n",
    "        schemas[p.name] = schema\n",
    "        row_counts[p.name] = meta.metadata.num_rows\n",
    "        # coletar alguns valores de exemplo das primeiras 5 linhas\n",
    "        table = meta.read_row_groups([0], columns=[f.name for f in schema]) if meta.metadata.num_row_groups>0 else meta.read()\n",
    "        head = table.slice(0, min(5, table.num_rows))\n",
    "        for f in schema:\n",
    "            col = head.column(f.name) if f.name in head.column_names else None\n",
    "            if col is not None:\n",
    "                vals = col.to_pylist()\n",
    "                example_values[p.name][f.name] = vals\n",
    "    except Exception as e:\n",
    "        errors[p.name] = str(e)\n",
    "\n",
    "# Determinar colunas comuns e tipos\n",
    "all_columns = [set(s.names) for s in schemas.values()]\n",
    "common_cols = set.intersection(*all_columns) if all_columns else set()\n",
    "\n",
    "# mapa de tipos por coluna (contagem)\n",
    "col_type_counter = defaultdict(Counter)\n",
    "for fname, schema in schemas.items():\n",
    "    for f in schema:\n",
    "        col_type_counter[f.name][str(f.type)] += 1\n",
    "\n",
    "print(\"\\nErros de leitura:\")\n",
    "if errors:\n",
    "    for k,v in errors.items():\n",
    "        print(f\"- {k}: {v}\")\n",
    "else:\n",
    "    print(\"Nenhum erro\")\n",
    "\n",
    "print(\"\\nColunas comuns (presentes em todos):\")\n",
    "print(sorted(common_cols))\n",
    "\n",
    "print(\"\\nTipos por coluna (top-1):\")\n",
    "for col, counter in sorted(col_type_counter.items()):\n",
    "    top = counter.most_common(1)[0]\n",
    "    print(f\"- {col}: {top[0]} (freq {top[1]}/{len(schemas)})\")\n",
    "\n",
    "print(\"\\nAmostras de valores (primeiras 5 linhas) para 2 arquivos:\")\n",
    "for fname in list(schemas.keys())[:2]:\n",
    "    print(f\"\\n== {fname} (linhas={row_counts.get(fname)}) ==\")\n",
    "    samples = example_values.get(fname, {})\n",
    "    for col, vals in list(samples.items())[:10]:\n",
    "        print(f\"  {col}: {vals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"file_count\": 31,\n",
      "  \"error_count\": 0,\n",
      "  \"error_files\": [],\n",
      "  \"has_ohlcv_actions_pattern\": 31,\n",
      "  \"sample_files\": [\n",
      "    \"_bvsp_ohlcv_actions_20120101_20250922.parquet\",\n",
      "    \"_gspc_ohlcv_actions_20120101_20250922.parquet\",\n",
      "    \"_tnx_ohlcv_actions_20120101_20250922.parquet\",\n",
      "    \"_vix_ohlcv_actions_20120101_20250922.parquet\",\n",
      "    \"abev3_sa_ohlcv_actions_20120101_20250922.parquet\"\n",
      "  ],\n",
      "  \"common_columns\": [\n",
      "    \"Adj Close\",\n",
      "    \"Close\",\n",
      "    \"Date\",\n",
      "    \"Dividends\",\n",
      "    \"High\",\n",
      "    \"Low\",\n",
      "    \"Open\",\n",
      "    \"Stock Splits\",\n",
      "    \"Volume\"\n",
      "  ],\n",
      "  \"common_columns_types\": {\n",
      "    \"Adj Close\": {\n",
      "      \"type\": \"double\",\n",
      "      \"freq\": 31,\n",
      "      \"total\": 31\n",
      "    },\n",
      "    \"Close\": {\n",
      "      \"type\": \"double\",\n",
      "      \"freq\": 31,\n",
      "      \"total\": 31\n",
      "    },\n",
      "    \"Date\": {\n",
      "      \"type\": \"timestamp[ns, tz=America/Sao_Paulo]\",\n",
      "      \"freq\": 25,\n",
      "      \"total\": 31\n",
      "    },\n",
      "    \"Dividends\": {\n",
      "      \"type\": \"double\",\n",
      "      \"freq\": 31,\n",
      "      \"total\": 31\n",
      "    },\n",
      "    \"High\": {\n",
      "      \"type\": \"double\",\n",
      "      \"freq\": 31,\n",
      "      \"total\": 31\n",
      "    },\n",
      "    \"Low\": {\n",
      "      \"type\": \"double\",\n",
      "      \"freq\": 31,\n",
      "      \"total\": 31\n",
      "    },\n",
      "    \"Open\": {\n",
      "      \"type\": \"double\",\n",
      "      \"freq\": 31,\n",
      "      \"total\": 31\n",
      "    },\n",
      "    \"Stock Splits\": {\n",
      "      \"type\": \"double\",\n",
      "      \"freq\": 31,\n",
      "      \"total\": 31\n",
      "    },\n",
      "    \"Volume\": {\n",
      "      \"type\": \"int64\",\n",
      "      \"freq\": 31,\n",
      "      \"total\": 31\n",
      "    }\n",
      "  },\n",
      "  \"sample_columns_by_file\": {\n",
      "    \"_bvsp_ohlcv_actions_20120101_20250922.parquet\": [\n",
      "      \"Open\",\n",
      "      \"High\",\n",
      "      \"Low\",\n",
      "      \"Close\",\n",
      "      \"Adj Close\",\n",
      "      \"Volume\",\n",
      "      \"Dividends\",\n",
      "      \"Stock Splits\",\n",
      "      \"Date\"\n",
      "    ],\n",
      "    \"_gspc_ohlcv_actions_20120101_20250922.parquet\": [\n",
      "      \"Open\",\n",
      "      \"High\",\n",
      "      \"Low\",\n",
      "      \"Close\",\n",
      "      \"Adj Close\",\n",
      "      \"Volume\",\n",
      "      \"Dividends\",\n",
      "      \"Stock Splits\",\n",
      "      \"Date\"\n",
      "    ]\n",
      "  },\n",
      "  \"name_patterns_counts\": {\n",
      "    \"_sa_\": 24,\n",
      "    \"_nyb_\": 0,\n",
      "    \"_metadata_\": 0,\n",
      "    \"_ohlcv_actions_\": 31\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Resumo compacto (JSON) das estruturas parquet\n",
    "import json, re\n",
    "\n",
    "def top_types_for(cols):\n",
    "    out = {}\n",
    "    for c in cols:\n",
    "        cnt = col_type_counter.get(c, None)\n",
    "        if cnt:\n",
    "            t, n = cnt.most_common(1)[0]\n",
    "            out[c] = {\"type\": t, \"freq\": n, \"total\": len(schemas)}\n",
    "    return out\n",
    "\n",
    "summary = {}\n",
    "summary[\"file_count\"] = len(files)\n",
    "summary[\"error_count\"] = len(errors)\n",
    "summary[\"error_files\"] = sorted(list(errors.keys()))[:5]\n",
    "summary[\"has_ohlcv_actions_pattern\"] = sum(1 for p in files if \"_ohlcv_actions_\" in p.name)\n",
    "\n",
    "names = [p.name for p in files]\n",
    "summary[\"sample_files\"] = names[:5]\n",
    "\n",
    "# Colunas comuns e tipos\n",
    "cc = sorted(list(common_cols))\n",
    "summary[\"common_columns\"] = cc\n",
    "summary[\"common_columns_types\"] = top_types_for(cc)\n",
    "\n",
    "# Amostra de colunas de 2 arquivos\n",
    "sample_cols = {}\n",
    "for fname in names[:2]:\n",
    "    sch = schemas.get(fname)\n",
    "    if sch:\n",
    "        sample_cols[fname] = sch.names\n",
    "summary[\"sample_columns_by_file\"] = sample_cols\n",
    "\n",
    "# Padrões por nome: conta tags tipo '_sa_', '_nyb_', etc\n",
    "patterns = {\n",
    "    \"_sa_\": sum(1 for n in names if \"_sa_\" in n),\n",
    "    \"_nyb_\": sum(1 for n in names if \"_nyb_\" in n),\n",
    "    \"_metadata_\": sum(1 for n in names if \"_metadata_\" in n),\n",
    "    \"_ohlcv_actions_\": sum(1 for n in names if \"_ohlcv_actions_\" in n),\n",
    "}\n",
    "summary[\"name_patterns_counts\"] = patterns\n",
    "\n",
    "print(json.dumps(summary, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos a ajustar (esperados 6): 6\n",
      "- _gspc_ohlcv_actions_20120101_20250922.parquet\n",
      "- _tnx_ohlcv_actions_20120101_20250922.parquet\n",
      "- _vix_ohlcv_actions_20120101_20250922.parquet\n",
      "- bz=f_ohlcv_actions_20120101_20250922.parquet\n",
      "- dx-y.nyb_ohlcv_actions_20120101_20250922.parquet\n",
      "- ewz_ohlcv_actions_20120101_20250922.parquet\n",
      "Ajustados e salvos: 6 em G:\\Drives compartilhados\\BOLSA_2026\\a_bolsa2026_gemini\\00_data\\02_processed\n",
      "  -> _gspc_ohlcv_actions_20120101_20250922.parquet\n",
      "  -> _tnx_ohlcv_actions_20120101_20250922.parquet\n",
      "  -> _vix_ohlcv_actions_20120101_20250922.parquet\n",
      "  -> bz=f_ohlcv_actions_20120101_20250922.parquet\n",
      "  -> dx-y.nyb_ohlcv_actions_20120101_20250922.parquet\n",
      "  -> ewz_ohlcv_actions_20120101_20250922.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_22572\\2409226319.py:69: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if pd.api.types.is_datetime64tz_dtype(d):\n",
      "C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_22572\\2409226319.py:69: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if pd.api.types.is_datetime64tz_dtype(d):\n",
      "C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_22572\\2409226319.py:69: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if pd.api.types.is_datetime64tz_dtype(d):\n",
      "C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_22572\\2409226319.py:69: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if pd.api.types.is_datetime64tz_dtype(d):\n",
      "C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_22572\\2409226319.py:69: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if pd.api.types.is_datetime64tz_dtype(d):\n",
      "C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_22572\\2409226319.py:69: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if pd.api.types.is_datetime64tz_dtype(d):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificação: 6/6 com Date em tz=America/Sao_Paulo\n"
     ]
    }
   ],
   "source": [
    "# Ajustar timezone para America/Sao_Paulo nos 6 arquivos divergentes e salvar em 00_data/02_processed\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "RAW_DIR = Path('G:/Drives compartilhados/BOLSA_2026/a_bolsa2026_gemini/00_data/01_raw')\n",
    "PROC_DIR = Path('G:/Drives compartilhados/BOLSA_2026/a_bolsa2026_gemini/00_data/02_processed')\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Utilizar resultados computados previamente (schemas, files) se existirem no kernel; caso contrário, recomputar leve\n",
    "try:\n",
    "    files\n",
    "    schemas\n",
    "except NameError:\n",
    "    from collections import Counter, defaultdict\n",
    "    import pyarrow.parquet as pq\n",
    "    files = sorted([p for p in RAW_DIR.glob('*.parquet')])\n",
    "    schemas = {}\n",
    "    for p in files:\n",
    "        meta = pq.ParquetFile(p)\n",
    "        schemas[p.name] = meta.schema_arrow\n",
    "\n",
    "# Identificar quais não têm tz America/Sao_Paulo\n",
    "not_sp = []\n",
    "for p in files:\n",
    "    sch = schemas.get(p.name)\n",
    "    if not sch:\n",
    "        continue\n",
    "    if 'Date' in sch.names:\n",
    "        f = next((f for f in sch if f.name == 'Date'), None)\n",
    "        t = str(f.type) if f else ''\n",
    "        if 'timestamp' in t and 'tz=America/Sao_Paulo' in t:\n",
    "            continue\n",
    "        if 'timestamp' in t:\n",
    "            not_sp.append(p)\n",
    "    else:\n",
    "        # Pode estar como index no parquet (sem coluna explícita)\n",
    "        not_sp.append(p)\n",
    "\n",
    "print(f\"Arquivos a ajustar (esperados 6): {len(not_sp)}\")\n",
    "for p in not_sp:\n",
    "    print('-', p.name)\n",
    "\n",
    "# Função de ajuste: garantir Date com tz America/Sao_Paulo\n",
    "import pytz\n",
    "sp_tz = pytz.timezone('America/Sao_Paulo')\n",
    "\n",
    "fixed = []\n",
    "for p in not_sp:\n",
    "    df = pd.read_parquet(p)  # pandas com pyarrow engine\n",
    "\n",
    "    # Descobrir a coluna/índice de data\n",
    "    date_col = None\n",
    "    for cand in ['Date', 'date', 'DATE', 'Datetime', 'datetime', 'Timestamp', 'timestamp']:\n",
    "        if cand in df.columns:\n",
    "            date_col = cand\n",
    "            break\n",
    "    if date_col is None:\n",
    "        # Tentar índice\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            df = df.reset_index().rename(columns={'index':'Date'})\n",
    "            date_col = 'Date'\n",
    "        else:\n",
    "            print(f\"[SKIP] Sem coluna/índice de data reconhecida: {p.name}\")\n",
    "            continue\n",
    "\n",
    "    d = pd.to_datetime(df[date_col], errors='coerce', utc=False)\n",
    "\n",
    "    if pd.api.types.is_datetime64tz_dtype(d):\n",
    "        # já tem tz, converter\n",
    "        df[date_col] = d.dt.tz_convert('America/Sao_Paulo')\n",
    "    else:\n",
    "        # naive -> assumir horário local SP\n",
    "        df[date_col] = d.dt.tz_localize('America/Sao_Paulo')\n",
    "\n",
    "    # Garantir nome padronizado 'Date'\n",
    "    if date_col != 'Date':\n",
    "        df = df.rename(columns={date_col: 'Date'})\n",
    "\n",
    "    out_path = PROC_DIR / p.name\n",
    "    df.to_parquet(out_path, engine='pyarrow', index=False)\n",
    "    fixed.append(out_path)\n",
    "\n",
    "print(f\"Ajustados e salvos: {len(fixed)} em {PROC_DIR}\")\n",
    "for q in fixed:\n",
    "    print('  ->', q.name)\n",
    "\n",
    "# Verificar tipos após escrita\n",
    "import pyarrow.parquet as pq\n",
    "ok = 0\n",
    "for q in fixed:\n",
    "    meta = pq.ParquetFile(q)\n",
    "    sch = meta.schema_arrow\n",
    "    f = next((f for f in sch if f.name == 'Date'), None)\n",
    "    if f and 'tz=America/Sao_Paulo' in str(f.type):\n",
    "        ok += 1\n",
    "print(f\"Verificação: {ok}/{len(fixed)} com Date em tz=America/Sao_Paulo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total arquivos a processar: 31\n",
      "Gravados 31 arquivos em G:\\Drives compartilhados\\BOLSA_2026\\a_bolsa2026_gemini\\00_data\\02_processed\n",
      "OK_COLS = True\n",
      "OK_TZ_SP = True\n"
     ]
    }
   ],
   "source": [
    "# Padronizar camada SILVER: schema consistente, tz America/Sao_Paulo, coluna Symbol, salvar em 00_data/02_processed\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "RAW_DIR = Path('G:/Drives compartilhados/BOLSA_2026/a_bolsa2026_gemini/00_data/01_raw')\n",
    "SILVER_DIR = Path('G:/Drives compartilhados/BOLSA_2026/a_bolsa2026_gemini/00_data/02_processed')\n",
    "SILVER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Lista alvo de colunas na ordem final\n",
    "TARGET_COLS = ['Symbol','Date','Open','High','Low','Close','Adj Close','Volume','Dividends','Stock Splits']\n",
    "\n",
    "# Extrair símbolo a partir do nome do arquivo\n",
    "# Exemplos: \"abev3_sa_ohlcv_actions_...\", \"_gspc_ohlcv_actions_...\"\n",
    "# Regra: pegar token inicial até o primeiro \"_ohlcv_actions_\"; remover prefixo '_' se houver; manter sufixo de mercado (ex.: _sa_)\n",
    "SYMBOL_RE = re.compile(r\"^(.*?)_ohlcv_actions_\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def infer_symbol(fname: str) -> str:\n",
    "    m = SYMBOL_RE.match(fname)\n",
    "    raw = m.group(1) if m else fname.split('.')[0]\n",
    "    # normalizar: tirar duplicidade de underscores, remover prefixo '_'\n",
    "    raw = raw.lstrip('_')\n",
    "    return raw.lower()\n",
    "\n",
    "\n",
    "def ensure_tz_sp(s: pd.Series) -> pd.Series:\n",
    "    s = pd.to_datetime(s, errors='coerce', utc=False)\n",
    "    if isinstance(s.dtype, pd.DatetimeTZDtype):\n",
    "        return s.dt.tz_convert('America/Sao_Paulo')\n",
    "    else:\n",
    "        return s.dt.tz_localize('America/Sao_Paulo')\n",
    "\n",
    "\n",
    "files = sorted(RAW_DIR.glob('*.parquet'))\n",
    "print(f\"Total arquivos a processar: {len(files)}\")\n",
    "\n",
    "written = []\n",
    "for p in files:\n",
    "    df = pd.read_parquet(p)\n",
    "\n",
    "    # Descobrir/Padronizar Date\n",
    "    date_col = None\n",
    "    for cand in ['Date','date','DATE','Datetime','datetime','Timestamp','timestamp']:\n",
    "        if cand in df.columns:\n",
    "            date_col = cand\n",
    "            break\n",
    "    if date_col is None and isinstance(df.index, pd.DatetimeIndex):\n",
    "        df = df.reset_index().rename(columns={'index':'Date'})\n",
    "        date_col = 'Date'\n",
    "    if date_col is None:\n",
    "        print(f\"[SKIP] {p.name}: sem coluna/índice de data.\")\n",
    "        continue\n",
    "\n",
    "    # Tipos numéricos / faltantes\n",
    "    for col in ['Open','High','Low','Close','Adj Close','Volume','Dividends','Stock Splits']:\n",
    "        if col in df.columns:\n",
    "            if col == 'Volume':\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')  # permitir NA, salva como int64 no parquet\n",
    "            else:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').astype('float64')\n",
    "        else:\n",
    "            # coluna ausente vira NA\n",
    "            df[col] = pd.Series([pd.NA]*len(df)) if col=='Volume' else pd.Series([float('nan')]*len(df))\n",
    "\n",
    "    # Garantir timezone America/Sao_Paulo\n",
    "    df[date_col] = ensure_tz_sp(df[date_col])\n",
    "    if date_col != 'Date':\n",
    "        df = df.rename(columns={date_col: 'Date'})\n",
    "\n",
    "    # Adicionar Symbol\n",
    "    symbol = infer_symbol(p.name)\n",
    "    df.insert(0, 'Symbol', symbol)\n",
    "\n",
    "    # Reordenar colunas\n",
    "    missing = [c for c in TARGET_COLS if c not in df.columns]\n",
    "    for c in missing:\n",
    "        if c == 'Volume' and c not in df.columns:\n",
    "            df[c] = pd.Series([pd.NA]*len(df))\n",
    "        elif c not in df.columns:\n",
    "            df[c] = float('nan')\n",
    "    df = df[TARGET_COLS]\n",
    "\n",
    "    # Escrever com pyarrow, mantendo tz na Date\n",
    "    out = SILVER_DIR / p.name\n",
    "    df.to_parquet(out, engine='pyarrow', index=False)\n",
    "    written.append(out)\n",
    "\n",
    "print(f\"Gravados {len(written)} arquivos em {SILVER_DIR}\")\n",
    "\n",
    "# Verificação: todos com schema idêntico e tz correta\n",
    "schemas = []\n",
    "for q in written:\n",
    "    meta = pq.ParquetFile(q)\n",
    "    sch = meta.schema_arrow\n",
    "    schemas.append((q.name, sch))\n",
    "\n",
    "# verificar nomes/ordem das colunas\n",
    "ok_cols = all(s.names == TARGET_COLS for _, s in schemas)\n",
    "# verificar tz da Date\n",
    "ok_tz = all('tz=America/Sao_Paulo' in str(next(f for f in s if f.name=='Date').type) for _, s in schemas)\n",
    "\n",
    "print('OK_COLS =', ok_cols)\n",
    "print('OK_TZ_SP =', ok_tz)\n",
    "if not ok_cols:\n",
    "    bad = [n for n,s in schemas if s.names != TARGET_COLS]\n",
    "    print('Arquivos com colunas fora do padrão:', bad[:5])\n",
    "if not ok_tz:\n",
    "    bad = []\n",
    "    for n, s in schemas:\n",
    "        f = next((f for f in s if f.name=='Date'), None)\n",
    "        if 'tz=America/Sao_Paulo' not in str(f.type):\n",
    "            bad.append(n)\n",
    "    print('Arquivos com timezone fora do padrão:', bad[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo: petr4_sa_ohlcv_actions_20120101_20250922.parquet\n",
      "Período: 2022-09-01 a 2022-09-30\n",
      "Linhas: 21\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Symbol",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Date",
         "rawType": "datetime64[ns, America/Sao_Paulo]",
         "type": "unknown"
        },
        {
         "name": "Open",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "High",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Low",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Close",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Adj Close",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Volume",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "Dividends",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Stock Splits",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "2de98e60-8a94-498c-b77f-8c4fd55e01b9",
       "rows": [
        [
         "0",
         "petr4_sa",
         "2022-09-01 00:00:00-03:00",
         "33.36000061035156",
         "33.95000076293945",
         "32.7599983215332",
         "33.849998474121094",
         "18.471288681030273",
         "69184700",
         "0.0",
         "0.0"
        ],
        [
         "1",
         "petr4_sa",
         "2022-09-02 00:00:00-03:00",
         "34.439998626708984",
         "34.56999969482422",
         "33.2599983215332",
         "33.41999816894531",
         "18.23664665222168",
         "81792300",
         "0.0",
         "0.0"
        ],
        [
         "2",
         "petr4_sa",
         "2022-09-05 00:00:00-03:00",
         "33.849998474121094",
         "34.11000061035156",
         "33.150001525878906",
         "33.34000015258789",
         "18.192995071411133",
         "76337600",
         "0.0",
         "0.0"
        ],
        [
         "3",
         "petr4_sa",
         "2022-09-06 00:00:00-03:00",
         "32.27000045776367",
         "32.59000015258789",
         "31.510000228881836",
         "32.099998474121094",
         "17.516345977783203",
         "111813500",
         "0.0",
         "0.0"
        ],
        [
         "4",
         "petr4_sa",
         "2022-09-08 00:00:00-03:00",
         "32.310001373291016",
         "32.779998779296875",
         "31.399999618530273",
         "31.799999237060547",
         "17.352642059326172",
         "63159800",
         "0.0",
         "0.0"
        ],
        [
         "5",
         "petr4_sa",
         "2022-09-09 00:00:00-03:00",
         "32.4900016784668",
         "32.66999816894531",
         "31.700000762939453",
         "31.790000915527344",
         "17.347187042236328",
         "49871900",
         "0.0",
         "0.0"
        ],
        [
         "6",
         "petr4_sa",
         "2022-09-12 00:00:00-03:00",
         "32.150001525878906",
         "32.72999954223633",
         "31.459999084472656",
         "31.479999542236328",
         "17.178028106689453",
         "71374500",
         "0.0",
         "0.0"
        ],
        [
         "7",
         "petr4_sa",
         "2022-09-13 00:00:00-03:00",
         "30.940000534057617",
         "31.440000534057617",
         "30.520000457763672",
         "30.649999618530273",
         "16.725109100341797",
         "88327800",
         "0.0",
         "0.0"
        ],
        [
         "8",
         "petr4_sa",
         "2022-09-14 00:00:00-03:00",
         "30.75",
         "31.43000030517578",
         "30.6299991607666",
         "31.1200008392334",
         "16.98158073425293",
         "49039700",
         "0.0",
         "0.0"
        ],
        [
         "9",
         "petr4_sa",
         "2022-09-15 00:00:00-03:00",
         "30.920000076293945",
         "31.229999542236328",
         "30.799999237060547",
         "31.059999465942383",
         "16.94883918762207",
         "45553200",
         "0.0",
         "0.0"
        ],
        [
         "10",
         "petr4_sa",
         "2022-09-16 00:00:00-03:00",
         "30.860000610351562",
         "30.950000762939453",
         "30.34000015258789",
         "30.780000686645508",
         "16.796049118041992",
         "107303600",
         "0.0",
         "0.0"
        ],
        [
         "11",
         "petr4_sa",
         "2022-09-19 00:00:00-03:00",
         "30.5",
         "31.3799991607666",
         "29.8799991607666",
         "31.270000457763672",
         "17.063432693481445",
         "65559900",
         "0.0",
         "0.0"
        ],
        [
         "12",
         "petr4_sa",
         "2022-09-20 00:00:00-03:00",
         "31.420000076293945",
         "31.600000381469727",
         "30.84000015258789",
         "31.09000015258789",
         "16.9652156829834",
         "67798200",
         "0.0",
         "0.0"
        ],
        [
         "13",
         "petr4_sa",
         "2022-09-21 00:00:00-03:00",
         "31.350000381469727",
         "31.469999313354492",
         "30.889999389648438",
         "31.170000076293945",
         "17.008865356445312",
         "58812200",
         "0.0",
         "0.0"
        ],
        [
         "14",
         "petr4_sa",
         "2022-09-22 00:00:00-03:00",
         "31.479999542236328",
         "32.08000183105469",
         "31.030000686645508",
         "31.940000534057617",
         "17.429040908813477",
         "69764300",
         "0.0",
         "0.0"
        ],
        [
         "15",
         "petr4_sa",
         "2022-09-23 00:00:00-03:00",
         "31.31999969482422",
         "31.389999389648438",
         "29.770000457763672",
         "29.940000534057617",
         "16.337678909301758",
         "138630800",
         "0.0",
         "0.0"
        ],
        [
         "16",
         "petr4_sa",
         "2022-09-26 00:00:00-03:00",
         "29.719999313354492",
         "30.079999923706055",
         "29.270000457763672",
         "29.290000915527344",
         "15.982987403869629",
         "42934600",
         "0.0",
         "0.0"
        ],
        [
         "17",
         "petr4_sa",
         "2022-09-27 00:00:00-03:00",
         "29.65999984741211",
         "29.8799991607666",
         "29.579999923706055",
         "29.670000076293945",
         "16.190343856811523",
         "71087600",
         "0.0",
         "0.0"
        ],
        [
         "18",
         "petr4_sa",
         "2022-09-28 00:00:00-03:00",
         "29.670000076293945",
         "29.81999969482422",
         "28.579999923706055",
         "29.270000457763672",
         "15.972070693969727",
         "104670700",
         "0.0",
         "0.0"
        ],
        [
         "19",
         "petr4_sa",
         "2022-09-29 00:00:00-03:00",
         "28.989999771118164",
         "29.399999618530273",
         "28.600000381469727",
         "29.309999465942383",
         "15.99389934539795",
         "87536500",
         "0.0",
         "0.0"
        ],
        [
         "20",
         "petr4_sa",
         "2022-09-30 00:00:00-03:00",
         "29.219999313354492",
         "30.489999771118164",
         "29.0",
         "29.799999237060547",
         "16.26128387451172",
         "121637100",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 21
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-01 00:00:00-03:00</td>\n",
       "      <td>33.360001</td>\n",
       "      <td>33.950001</td>\n",
       "      <td>32.759998</td>\n",
       "      <td>33.849998</td>\n",
       "      <td>18.471289</td>\n",
       "      <td>69184700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-02 00:00:00-03:00</td>\n",
       "      <td>34.439999</td>\n",
       "      <td>34.570000</td>\n",
       "      <td>33.259998</td>\n",
       "      <td>33.419998</td>\n",
       "      <td>18.236647</td>\n",
       "      <td>81792300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-05 00:00:00-03:00</td>\n",
       "      <td>33.849998</td>\n",
       "      <td>34.110001</td>\n",
       "      <td>33.150002</td>\n",
       "      <td>33.340000</td>\n",
       "      <td>18.192995</td>\n",
       "      <td>76337600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-06 00:00:00-03:00</td>\n",
       "      <td>32.270000</td>\n",
       "      <td>32.590000</td>\n",
       "      <td>31.510000</td>\n",
       "      <td>32.099998</td>\n",
       "      <td>17.516346</td>\n",
       "      <td>111813500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-08 00:00:00-03:00</td>\n",
       "      <td>32.310001</td>\n",
       "      <td>32.779999</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>31.799999</td>\n",
       "      <td>17.352642</td>\n",
       "      <td>63159800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-09 00:00:00-03:00</td>\n",
       "      <td>32.490002</td>\n",
       "      <td>32.669998</td>\n",
       "      <td>31.700001</td>\n",
       "      <td>31.790001</td>\n",
       "      <td>17.347187</td>\n",
       "      <td>49871900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-12 00:00:00-03:00</td>\n",
       "      <td>32.150002</td>\n",
       "      <td>32.730000</td>\n",
       "      <td>31.459999</td>\n",
       "      <td>31.480000</td>\n",
       "      <td>17.178028</td>\n",
       "      <td>71374500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-13 00:00:00-03:00</td>\n",
       "      <td>30.940001</td>\n",
       "      <td>31.440001</td>\n",
       "      <td>30.520000</td>\n",
       "      <td>30.650000</td>\n",
       "      <td>16.725109</td>\n",
       "      <td>88327800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-14 00:00:00-03:00</td>\n",
       "      <td>30.750000</td>\n",
       "      <td>31.430000</td>\n",
       "      <td>30.629999</td>\n",
       "      <td>31.120001</td>\n",
       "      <td>16.981581</td>\n",
       "      <td>49039700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-15 00:00:00-03:00</td>\n",
       "      <td>30.920000</td>\n",
       "      <td>31.230000</td>\n",
       "      <td>30.799999</td>\n",
       "      <td>31.059999</td>\n",
       "      <td>16.948839</td>\n",
       "      <td>45553200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-16 00:00:00-03:00</td>\n",
       "      <td>30.860001</td>\n",
       "      <td>30.950001</td>\n",
       "      <td>30.340000</td>\n",
       "      <td>30.780001</td>\n",
       "      <td>16.796049</td>\n",
       "      <td>107303600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-19 00:00:00-03:00</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>31.379999</td>\n",
       "      <td>29.879999</td>\n",
       "      <td>31.270000</td>\n",
       "      <td>17.063433</td>\n",
       "      <td>65559900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-20 00:00:00-03:00</td>\n",
       "      <td>31.420000</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>30.840000</td>\n",
       "      <td>31.090000</td>\n",
       "      <td>16.965216</td>\n",
       "      <td>67798200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-21 00:00:00-03:00</td>\n",
       "      <td>31.350000</td>\n",
       "      <td>31.469999</td>\n",
       "      <td>30.889999</td>\n",
       "      <td>31.170000</td>\n",
       "      <td>17.008865</td>\n",
       "      <td>58812200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-22 00:00:00-03:00</td>\n",
       "      <td>31.480000</td>\n",
       "      <td>32.080002</td>\n",
       "      <td>31.030001</td>\n",
       "      <td>31.940001</td>\n",
       "      <td>17.429041</td>\n",
       "      <td>69764300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-23 00:00:00-03:00</td>\n",
       "      <td>31.320000</td>\n",
       "      <td>31.389999</td>\n",
       "      <td>29.770000</td>\n",
       "      <td>29.940001</td>\n",
       "      <td>16.337679</td>\n",
       "      <td>138630800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-26 00:00:00-03:00</td>\n",
       "      <td>29.719999</td>\n",
       "      <td>30.080000</td>\n",
       "      <td>29.270000</td>\n",
       "      <td>29.290001</td>\n",
       "      <td>15.982987</td>\n",
       "      <td>42934600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-27 00:00:00-03:00</td>\n",
       "      <td>29.660000</td>\n",
       "      <td>29.879999</td>\n",
       "      <td>29.580000</td>\n",
       "      <td>29.670000</td>\n",
       "      <td>16.190344</td>\n",
       "      <td>71087600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-28 00:00:00-03:00</td>\n",
       "      <td>29.670000</td>\n",
       "      <td>29.820000</td>\n",
       "      <td>28.580000</td>\n",
       "      <td>29.270000</td>\n",
       "      <td>15.972071</td>\n",
       "      <td>104670700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-29 00:00:00-03:00</td>\n",
       "      <td>28.990000</td>\n",
       "      <td>29.400000</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>29.309999</td>\n",
       "      <td>15.993899</td>\n",
       "      <td>87536500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>petr4_sa</td>\n",
       "      <td>2022-09-30 00:00:00-03:00</td>\n",
       "      <td>29.219999</td>\n",
       "      <td>30.490000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.799999</td>\n",
       "      <td>16.261284</td>\n",
       "      <td>121637100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Symbol                      Date       Open       High        Low  \\\n",
       "0   petr4_sa 2022-09-01 00:00:00-03:00  33.360001  33.950001  32.759998   \n",
       "1   petr4_sa 2022-09-02 00:00:00-03:00  34.439999  34.570000  33.259998   \n",
       "2   petr4_sa 2022-09-05 00:00:00-03:00  33.849998  34.110001  33.150002   \n",
       "3   petr4_sa 2022-09-06 00:00:00-03:00  32.270000  32.590000  31.510000   \n",
       "4   petr4_sa 2022-09-08 00:00:00-03:00  32.310001  32.779999  31.400000   \n",
       "5   petr4_sa 2022-09-09 00:00:00-03:00  32.490002  32.669998  31.700001   \n",
       "6   petr4_sa 2022-09-12 00:00:00-03:00  32.150002  32.730000  31.459999   \n",
       "7   petr4_sa 2022-09-13 00:00:00-03:00  30.940001  31.440001  30.520000   \n",
       "8   petr4_sa 2022-09-14 00:00:00-03:00  30.750000  31.430000  30.629999   \n",
       "9   petr4_sa 2022-09-15 00:00:00-03:00  30.920000  31.230000  30.799999   \n",
       "10  petr4_sa 2022-09-16 00:00:00-03:00  30.860001  30.950001  30.340000   \n",
       "11  petr4_sa 2022-09-19 00:00:00-03:00  30.500000  31.379999  29.879999   \n",
       "12  petr4_sa 2022-09-20 00:00:00-03:00  31.420000  31.600000  30.840000   \n",
       "13  petr4_sa 2022-09-21 00:00:00-03:00  31.350000  31.469999  30.889999   \n",
       "14  petr4_sa 2022-09-22 00:00:00-03:00  31.480000  32.080002  31.030001   \n",
       "15  petr4_sa 2022-09-23 00:00:00-03:00  31.320000  31.389999  29.770000   \n",
       "16  petr4_sa 2022-09-26 00:00:00-03:00  29.719999  30.080000  29.270000   \n",
       "17  petr4_sa 2022-09-27 00:00:00-03:00  29.660000  29.879999  29.580000   \n",
       "18  petr4_sa 2022-09-28 00:00:00-03:00  29.670000  29.820000  28.580000   \n",
       "19  petr4_sa 2022-09-29 00:00:00-03:00  28.990000  29.400000  28.600000   \n",
       "20  petr4_sa 2022-09-30 00:00:00-03:00  29.219999  30.490000  29.000000   \n",
       "\n",
       "        Close  Adj Close     Volume  Dividends  Stock Splits  \n",
       "0   33.849998  18.471289   69184700        0.0           0.0  \n",
       "1   33.419998  18.236647   81792300        0.0           0.0  \n",
       "2   33.340000  18.192995   76337600        0.0           0.0  \n",
       "3   32.099998  17.516346  111813500        0.0           0.0  \n",
       "4   31.799999  17.352642   63159800        0.0           0.0  \n",
       "5   31.790001  17.347187   49871900        0.0           0.0  \n",
       "6   31.480000  17.178028   71374500        0.0           0.0  \n",
       "7   30.650000  16.725109   88327800        0.0           0.0  \n",
       "8   31.120001  16.981581   49039700        0.0           0.0  \n",
       "9   31.059999  16.948839   45553200        0.0           0.0  \n",
       "10  30.780001  16.796049  107303600        0.0           0.0  \n",
       "11  31.270000  17.063433   65559900        0.0           0.0  \n",
       "12  31.090000  16.965216   67798200        0.0           0.0  \n",
       "13  31.170000  17.008865   58812200        0.0           0.0  \n",
       "14  31.940001  17.429041   69764300        0.0           0.0  \n",
       "15  29.940001  16.337679  138630800        0.0           0.0  \n",
       "16  29.290001  15.982987   42934600        0.0           0.0  \n",
       "17  29.670000  16.190344   71087600        0.0           0.0  \n",
       "18  29.270000  15.972071  104670700        0.0           0.0  \n",
       "19  29.309999  15.993899   87536500        0.0           0.0  \n",
       "20  29.799999  16.261284  121637100        0.0           0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar dados de PETR4 em setembro/2022 (camada silver)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Montar caminho para 02_processed usando SHARED_DRIVES se disponível\n",
    "try:\n",
    "    base = Path(SHARED_DRIVES) / 'BOLSA_2026' / 'a_bolsa2026_gemini'\n",
    "    proc_dir = base / '00_data' / '02_processed'\n",
    "    assert proc_dir.exists()\n",
    "except Exception:\n",
    "    proc_dir = Path(r'G:/Drives compartilhados/BOLSA_2026/a_bolsa2026_gemini/00_data/02_processed')\n",
    "\n",
    "# Localizar arquivo silver de PETR4\n",
    "petr4_file = next(proc_dir.glob('petr4_sa_ohlcv_actions_*.parquet'))\n",
    "\n",
    "# Ler dados\n",
    "df = pd.read_parquet(petr4_file)\n",
    "\n",
    "# Garantir Date com tz America/Sao_Paulo (deveria já estar, mas deixamos robusto)\n",
    "if not isinstance(df['Date'].dtype, pd.DatetimeTZDtype):\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.tz_localize('America/Sao_Paulo')\n",
    "\n",
    "# Filtro de setembro de 2022 (intervalo fechado-aberto)\n",
    "start = pd.Timestamp('2022-09-01', tz='America/Sao_Paulo')\n",
    "end = pd.Timestamp('2022-10-01', tz='America/Sao_Paulo')\n",
    "mask = (df['Date'] >= start) & (df['Date'] < end)\n",
    "sep22 = df.loc[mask].sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "print('Arquivo:', petr4_file.name)\n",
    "print('Período:', start.date(), 'a', (end - pd.Timedelta(seconds=1)).date())\n",
    "print('Linhas:', len(sep22))\n",
    "sep22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banco de Dados (Postgres/SQLite) – Camada Silver → SQL\n",
    "\n",
    "Vamos criar um schema SQL para persistir os dados deduplicados e com timezone America/Sao_Paulo, e carregar todos os 31 Parquets da pasta 02_processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB_URL = sqlite:///G:/Drives compartilhados/BOLSA_2026/a_bolsa2026_gemini/00_data/03_final/ohlcv.db\n"
     ]
    }
   ],
   "source": [
    "# Configuração de conexão (usa Postgres via env DATABASE_URL; se não setado, cai para SQLite local)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ex.: DATABASE_URL=postgresql+psycopg2://user:pass@host:5432/dbname\n",
    "DB_URL = os.getenv('DATABASE_URL')\n",
    "if not DB_URL:\n",
    "    # fallback local para validação\n",
    "    DB_URL = 'sqlite:///G:/Drives compartilhados/BOLSA_2026/a_bolsa2026_gemini/00_data/03_final/ohlcv.db'\n",
    "    Path('G:/Drives compartilhados/BOLSA_2026/a_bolsa2026_gemini/00_data/03_final').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('DB_URL =', DB_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela ohlcv_daily pronta.\n"
     ]
    }
   ],
   "source": [
    "# Criar engine SQLAlchemy e schema da tabela\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import text\n",
    "\n",
    "engine = sa.create_engine(DB_URL, future=True)\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    # Postgres: timestamptz; SQLite: armazena texto ISO8601 com tz ou timestamp naive (vamos manter texto ISO para portabilidade)\n",
    "    # Chave primária: (symbol, date)\n",
    "    conn.execute(text(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS ohlcv_daily (\n",
    "        symbol TEXT NOT NULL,\n",
    "        date   TEXT NOT NULL,\n",
    "        open   DOUBLE PRECISION,\n",
    "        high   DOUBLE PRECISION,\n",
    "        low    DOUBLE PRECISION,\n",
    "        close  DOUBLE PRECISION,\n",
    "        adj_close DOUBLE PRECISION,\n",
    "        volume BIGINT,\n",
    "        dividends DOUBLE PRECISION,\n",
    "        stock_splits DOUBLE PRECISION,\n",
    "        PRIMARY KEY (symbol, date)\n",
    "    )\n",
    "    \"\"\"))\n",
    "\n",
    "print('Tabela ohlcv_daily pronta.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos a carregar: 31\n",
      "_bvsp_ohlcv_actions_20120101_20250922.parquet: upsert 3400 linhas\n",
      "_gspc_ohlcv_actions_20120101_20250922.parquet: upsert 3449 linhas\n",
      "_tnx_ohlcv_actions_20120101_20250922.parquet: upsert 3448 linhas\n",
      "_vix_ohlcv_actions_20120101_20250922.parquet: upsert 3449 linhas\n",
      "abev3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "b3sa3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "bbas3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "bz=f_ohlcv_actions_20120101_20250922.parquet: upsert 3432 linhas\n",
      "cple6_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3408 linhas\n",
      "csna3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "dx-y.nyb_ohlcv_actions_20120101_20250922.parquet: upsert 3450 linhas\n",
      "elet3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "ewz_ohlcv_actions_20120101_20250922.parquet: upsert 3449 linhas\n",
      "ggbr4_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "hapv3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 1840 linhas\n",
      "itub4_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "lren3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "petr4_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "prio3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "pssa3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "rail3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 2606 linhas\n",
      "rdor3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 1187 linhas\n",
      "sbsp3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "suzb3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "taee11_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "tims3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "tots3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "ugpa3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "vale3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "vivt3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "wege3_sa_ohlcv_actions_20120101_20250922.parquet: upsert 3409 linhas\n",
      "Total de linhas upsertadas: 101298\n"
     ]
    }
   ],
   "source": [
    "# Carregar todos os Parquets de 02_processed para o banco (upsert deduplicado)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import text\n",
    "\n",
    "SILVER_DIR = Path('G:/Drives compartilhados/BOLSA_2026/a_bolsa2026_gemini/00_data/02_processed')\n",
    "files = sorted(SILVER_DIR.glob('*.parquet'))\n",
    "print('Arquivos a carregar:', len(files))\n",
    "\n",
    "dialect = sa.create_engine(DB_URL).dialect.name\n",
    "is_pg = (dialect == 'postgresql')\n",
    "\n",
    "# Preparar comandos de upsert específicos por SGBD\n",
    "if is_pg:\n",
    "    upsert_sql = text(\"\"\"\n",
    "        INSERT INTO ohlcv_daily (symbol, date, open, high, low, close, adj_close, volume, dividends, stock_splits)\n",
    "        VALUES (:symbol, :date, :open, :high, :low, :close, :adj_close, :volume, :dividends, :stock_splits)\n",
    "        ON CONFLICT (symbol, date) DO UPDATE SET\n",
    "            open=EXCLUDED.open,\n",
    "            high=EXCLUDED.high,\n",
    "            low=EXCLUDED.low,\n",
    "            close=EXCLUDED.close,\n",
    "            adj_close=EXCLUDED.adj_close,\n",
    "            volume=EXCLUDED.volume,\n",
    "            dividends=EXCLUDED.dividends,\n",
    "            stock_splits=EXCLUDED.stock_splits\n",
    "    \"\"\")\n",
    "else:\n",
    "    # SQLite\n",
    "    upsert_sql = text(\"\"\"\n",
    "        INSERT OR REPLACE INTO ohlcv_daily (symbol, date, open, high, low, close, adj_close, volume, dividends, stock_splits)\n",
    "        VALUES (:symbol, :date, :open, :high, :low, :close, :adj_close, :volume, :dividends, :stock_splits)\n",
    "    \"\"\")\n",
    "\n",
    "engine = sa.create_engine(DB_URL, future=True)\n",
    "\n",
    "def to_rows(df: pd.DataFrame):\n",
    "    # Garantir colunas e tipos\n",
    "    cols = ['Symbol','Date','Open','High','Low','Close','Adj Close','Volume','Dividends','Stock Splits']\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "    # Timezone: persistir ISO 8601 com offset (-03:00) mantendo America/Sao_Paulo\n",
    "    dt = df['Date']\n",
    "    if not isinstance(dt.dtype, pd.DatetimeTZDtype):\n",
    "        dt = pd.to_datetime(dt, errors='coerce').dt.tz_localize('America/Sao_Paulo')\n",
    "    else:\n",
    "        dt = dt.dt.tz_convert('America/Sao_Paulo')\n",
    "    date_iso = dt.apply(lambda x: x.isoformat() if pd.notnull(x) else None)\n",
    "\n",
    "    # Deduplicar por (Symbol, Date)\n",
    "    tmp = df.copy()\n",
    "    tmp['__date_iso__'] = date_iso\n",
    "    tmp = tmp.dropna(subset=['__date_iso__'])\n",
    "    tmp = tmp.sort_values(['Symbol','__date_iso__']).drop_duplicates(['Symbol','__date_iso__'], keep='last')\n",
    "\n",
    "    # Construir DataFrame final com nomes de colunas do SQL\n",
    "    out = pd.DataFrame({\n",
    "        'symbol': tmp['Symbol'].astype(str).str.lower(),\n",
    "        'date': tmp['__date_iso__'],\n",
    "        'open': pd.to_numeric(tmp['Open'], errors='coerce'),\n",
    "        'high': pd.to_numeric(tmp['High'], errors='coerce'),\n",
    "        'low': pd.to_numeric(tmp['Low'], errors='coerce'),\n",
    "        'close': pd.to_numeric(tmp['Close'], errors='coerce'),\n",
    "        'adj_close': pd.to_numeric(tmp['Adj Close'], errors='coerce'),\n",
    "        'volume': pd.to_numeric(tmp['Volume'], errors='coerce').astype('Int64'),\n",
    "        'dividends': pd.to_numeric(tmp['Dividends'], errors='coerce'),\n",
    "        'stock_splits': pd.to_numeric(tmp['Stock Splits'], errors='coerce'),\n",
    "    })\n",
    "\n",
    "    # Converter NaN/<NA> para None (para bind SQL)\n",
    "    out = out.astype(object).where(pd.notnull(out), None)\n",
    "    return out.to_dict(orient='records')\n",
    "\n",
    "loaded = 0\n",
    "with engine.begin() as conn:\n",
    "    for p in files:\n",
    "        df = pd.read_parquet(p)\n",
    "        # Assegurar presença da coluna Symbol no silver\n",
    "        if 'Symbol' not in df.columns:\n",
    "            # Inferir do nome, fallback\n",
    "            sym = p.name.split('_ohlcv_actions_')[0].lstrip('_').lower()\n",
    "            df.insert(0, 'Symbol', sym)\n",
    "        rows = to_rows(df)\n",
    "        if rows:\n",
    "            conn.execute(upsert_sql, rows)\n",
    "            loaded += len(rows)\n",
    "        print(f\"{p.name}: upsert {len(rows)} linhas\")\n",
    "\n",
    "print('Total de linhas upsertadas:', loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total linhas: 101298\n",
      "('abev3_sa', '2012-01-02T00:00:00-02:00', '2025-09-19T00:00:00-03:00', 3409)\n",
      "('b3sa3_sa', '2012-01-02T00:00:00-02:00', '2025-09-19T00:00:00-03:00', 3409)\n",
      "('bbas3_sa', '2012-01-02T00:00:00-02:00', '2025-09-19T00:00:00-03:00', 3409)\n",
      "('bvsp', '2012-01-03T00:00:00-02:00', '2025-09-19T00:00:00-03:00', 3400)\n",
      "('bz=f', '2012-01-03T03:00:00-02:00', '2025-09-19T01:00:00-03:00', 3432)\n",
      "('cple6_sa', '2012-01-02T00:00:00-02:00', '2025-09-19T00:00:00-03:00', 3408)\n",
      "('csna3_sa', '2012-01-02T00:00:00-02:00', '2025-09-19T00:00:00-03:00', 3409)\n",
      "('dx-y.nyb', '2012-01-03T03:00:00-02:00', '2025-09-19T01:00:00-03:00', 3450)\n",
      "('elet3_sa', '2012-01-02T00:00:00-02:00', '2025-09-19T00:00:00-03:00', 3409)\n",
      "('ewz', '2012-01-03T03:00:00-02:00', '2025-09-19T01:00:00-03:00', 3449)\n"
     ]
    }
   ],
   "source": [
    "# Verificações rápidas no banco: contagem total e amostra por símbolo\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import text\n",
    "\n",
    "engine = sa.create_engine(DB_URL, future=True)\n",
    "with engine.begin() as conn:\n",
    "    total = conn.execute(text('SELECT COUNT(*) FROM ohlcv_daily')).scalar_one()\n",
    "    print('Total linhas:', total)\n",
    "    rows = conn.execute(text('SELECT symbol, MIN(date), MAX(date), COUNT(*) FROM ohlcv_daily GROUP BY symbol ORDER BY symbol LIMIT 10')).fetchall()\n",
    "    for r in rows:\n",
    "        print(r)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMFrPtlHxn/1w9oMI2tL2r1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
