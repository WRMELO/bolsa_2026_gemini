{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb1edbe7",
   "metadata": {},
   "source": [
    "# Offline RL (fase SELL/HOLD) — preparação de s_t e D_t + Bandit baseline\n",
    "\n",
    "Nesta seção:\n",
    "- Preparamos estados s_t (apenas passado) a partir de `gold_tabular.csv` (Close & Volume — e CLV se houver High/Low).\n",
    "- Calculamos D_t: pior queda futura em 1/3/5 dias com pesos w=(1.0, 0.6, 0.3) — usado só para recompensa/rótulo, nunca como feature.\n",
    "- Baseline contextual: regressão logística (π(SELL|s)) com partição walk-forward e normalização apenas no treino.\n",
    "- Simulação com quarentena Q=5 para métricas: recall de quedas grandes, precisão, perda média evitada, turnover e P&L líquido (custos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb792afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54d57b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Bandit (LogReg) — Val: prec=0.600 rec=0.001\n",
      "Baseline Bandit (LogReg) — Test: prec=0.500 rec=0.001\n",
      "Sim/Val: {'recall_big': 0.0003572704537334762, 'precision': 0.6666666666666666, 'turnover': 0.00018158707100054475, 'net_PnL_proxy': -0.06482440203951295}\n",
      "Sim/Test: {'recall_big': 0.0005583472920156337, 'precision': 0.6, 'turnover': 0.00030350855894136213, 'net_PnL_proxy': -0.7594226656277075}\n"
     ]
    }
   ],
   "source": [
    "# Preparar s_t e D_t; Baseline Bandit SELL/HOLD usando GOLD-RL recém-criado\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Configs principais\n",
    "ALPHA = 1.0   # aversão à perda em r(HOLD) = -α·D_t\n",
    "COST  = 0.0015  # custo de SELL (slippage+corretagem) em fração (ex: 15 bps)\n",
    "Q     = 5     # quarentena em pregões\n",
    "W     = np.array([1.0, 0.6, 0.3])  # pesos para janelas 1/3/5\n",
    "AHEAD = [1,2,3,4,5]\n",
    "\n",
    "# 1) Carregar gold_rl_tabular.csv (produzido no bloco anterior)\n",
    "csv_path = Path(r\"G:\\Drives compartilhados\\BOLSA_2026\\a_bolsa2026_gemini\\00_data\\03_final\\gold_rl_tabular.csv\")\n",
    "if not csv_path.exists():\n",
    "    raise FileNotFoundError(f\"gold_rl_tabular.csv não encontrado: {csv_path}\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Normalizar nomes mínimos\n",
    "lower_map = {c.lower(): c for c in df.columns}\n",
    "col_date   = next((lower_map[c] for c in ['date','data','session','trading_day'] if c in lower_map), None)\n",
    "col_ticker = next((lower_map[c] for c in ['ticker','symbol','ativo'] if c in lower_map), None)\n",
    "\n",
    "if col_date != 'date':\n",
    "    df = df.rename(columns={col_date:'date'})\n",
    "if col_ticker != 'ticker':\n",
    "    df = df.rename(columns={col_ticker:'ticker'})\n",
    "\n",
    "# Parse e sort\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(['ticker','date']).reset_index(drop=True)\n",
    "\n",
    "# 2) D_t: pior retorno futuro observado em 1/3/5 dias à frente\n",
    "for k in AHEAD:\n",
    "    df[f'fwd{k}'] = df.groupby('ticker')['close'].pct_change(-k)\n",
    "\n",
    "df['D1'] = df['fwd1']\n",
    "df['D3'] = df[['fwd1','fwd2','fwd3']].min(axis=1)\n",
    "df['D5'] = df[['fwd1','fwd2','fwd3','fwd4','fwd5']].min(axis=1)\n",
    "\n",
    "df['D_t'] = W[0]*df['D1'] + W[1]*df['D3'] + W[2]*df['D5']\n",
    "\n",
    "# 3) Walk-forward simples (70/15/15 por data)\n",
    "all_dates = np.sort(df['date'].dropna().unique())\n",
    "tr_end = all_dates[int(0.7*len(all_dates))]\n",
    "va_end = all_dates[int(0.85*len(all_dates))]\n",
    "\n",
    "train = df[df['date'] <= tr_end].copy()\n",
    "val   = df[(df['date'] > tr_end) & (df['date'] <= va_end)].copy()\n",
    "test  = df[df['date'] > va_end].copy()\n",
    "\n",
    "# 4) Rotulagem do baseline e treino\n",
    "# Critério heurístico: SELL se (1+α)*D_t > COST\n",
    "for part in (train, val, test):\n",
    "    part['sell_label'] = (part['D_t'] * (1.0 + ALPHA) > COST)\n",
    "\n",
    "feature_cols = ['z1','z2','z3','z5','vol21','vol21_pct','rvol_pct','rvol_chg','clv','dist_peak20_sigma','pct_z2_le_m1','med_vol21']\n",
    "# Garantir que as colunas existam (CLV pode ser NaN)\n",
    "feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "\n",
    "Xtr, Xva, Xte = train[feature_cols], val[feature_cols], test[feature_cols]\n",
    "ytr, yva, yte = train['sell_label'], val['sell_label'], test['sell_label']\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "Xtr_, Xva_, Xte_ = Xtr.fillna(0.0), Xva.fillna(0.0), Xte.fillna(0.0)\n",
    "pipe.fit(Xtr_, ytr)\n",
    "va_pred, te_pred = pipe.predict(Xva_), pipe.predict(Xte_)\n",
    "\n",
    "print(\"Baseline Bandit (LogReg) — Val: prec=%.3f rec=%.3f\" % (precision_score(yva, va_pred, zero_division=0), recall_score(yva, va_pred, zero_division=0)))\n",
    "print(\"Baseline Bandit (LogReg) — Test: prec=%.3f rec=%.3f\" % (precision_score(yte, te_pred, zero_division=0), recall_score(yte, te_pred, zero_division=0)))\n",
    "\n",
    "# 5) Simulação com quarentena Q\n",
    "val['prob'] = pipe.predict_proba(Xva_)[:,1]\n",
    "test['prob'] = pipe.predict_proba(Xte_)[:,1]\n",
    "\n",
    "THR = 0.5\n",
    "\n",
    "def simulate(df_part: pd.DataFrame, thr=THR):\n",
    "    dfp = df_part.sort_values(['ticker','date']).copy()\n",
    "    dfp['action'] = 'HOLD'\n",
    "    dfp['cost'] = 0.0\n",
    "    dfp['avoid'] = 0.0\n",
    "    for tk, g in dfp.groupby('ticker'):\n",
    "        q_until = pd.Timestamp.min\n",
    "        idxs = g.index.tolist()\n",
    "        for idx in idxs:\n",
    "            d = dfp.at[idx, 'date']\n",
    "            act = 'HOLD'\n",
    "            cst = 0.0\n",
    "            if d >= q_until and dfp.at[idx, 'prob'] >= thr:\n",
    "                act = 'SELL'\n",
    "                cst = COST\n",
    "                q_until = d + pd.tseries.offsets.BDay(Q)\n",
    "            Dt = dfp.at[idx, 'D_t']\n",
    "            avoid = (-Dt) if (act=='SELL' and pd.notna(Dt)) else 0.0\n",
    "            dfp.at[idx, 'action'] = act\n",
    "            dfp.at[idx, 'cost'] = cst\n",
    "            dfp.at[idx, 'avoid'] = avoid\n",
    "    sold = (dfp['action']=='SELL')\n",
    "    recall_big = float(np.mean(dfp.loc[dfp['sell_label'], 'action']=='SELL')) if dfp['sell_label'].any() else 0.0\n",
    "    precision  = float(np.mean(dfp.loc[sold, 'sell_label'])) if sold.any() else 0.0\n",
    "    total_cost = float(dfp['cost'].sum())\n",
    "    total_avoid= float(dfp['avoid'].sum())\n",
    "    turnover   = float(sold.mean())\n",
    "    return {\n",
    "        'recall_big': recall_big,\n",
    "        'precision': precision,\n",
    "        'turnover': turnover,\n",
    "        'net_PnL_proxy': total_avoid - total_cost\n",
    "    }\n",
    "\n",
    "val_metrics = simulate(val)\n",
    "test_metrics = simulate(test)\n",
    "print(\"Sim/Val:\", val_metrics)\n",
    "print(\"Sim/Test:\", test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc85a9",
   "metadata": {},
   "source": [
    "# GOLD-RL a partir de Silver (Close & Volume)\n",
    "\n",
    "Este bloco constrói um dataset tabular para RL (SELL/HOLD) diretamente de:\n",
    "- `00_data/02_curado/silver_close.parquet`\n",
    "- `00_data/02_curado/silver_volume.parquet`\n",
    "\n",
    "Com máscara de datas B3 (via índice do BVSP quando disponível ou união de datas presentes), computando por ticker e dia apenas-features de passado:\n",
    "- z-scores dos retornos (1/3/5d) usando janela 252d (mín. 60)\n",
    "- distância ao pico 20d em σ\n",
    "- vol 21d e seu percentil rolling (252d)\n",
    "- RVOL (percentil rolling 252d e variação diária)\n",
    "- CLV (se houver High/Low)\n",
    "\n",
    "E dois sinais cross-section por dia:\n",
    "- % de papéis com z2 ≤ −1\n",
    "- mediana de vol21\n",
    "\n",
    "Saídas: `00_data/03_final/gold_rl_tabular.parquet` e `.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7658edb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_40964\\2925301448.py:98: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  feat = panel.groupby('ticker', group_keys=False).apply(per_ticker_features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo: G:\\Drives compartilhados\\BOLSA_2026\\a_bolsa2026_gemini\\00_data\\03_final\\gold_rl_tabular.parquet e G:\\Drives compartilhados\\BOLSA_2026\\a_bolsa2026_gemini\\00_data\\03_final\\gold_rl_tabular.csv\n"
     ]
    }
   ],
   "source": [
    "# Construir GOLD-RL (tabular) de silver_close/volume com máscara B3 e features pedidas\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "CLOSE_PATH  = Path(r\"G:\\Drives compartilhados\\BOLSA_2026\\a_bolsa2026_gemini\\00_data\\02_curado\\silver_close.parquet\")\n",
    "VOLUME_PATH = Path(r\"G:\\Drives compartilhados\\BOLSA_2026\\a_bolsa2026_gemini\\00_data\\02_curado\\silver_volume.parquet\")\n",
    "OUT_DIR     = Path(r\"G:\\Drives compartilhados\\BOLSA_2026\\a_bolsa2026_gemini\\00_data\\03_final\")\n",
    "\n",
    "if not CLOSE_PATH.exists() or not VOLUME_PATH.exists():\n",
    "    raise FileNotFoundError(\"silver_close.parquet ou silver_volume.parquet não encontrados em 00_data/02_curado.\")\n",
    "\n",
    "close_wide  = pd.read_parquet(CLOSE_PATH)\n",
    "volume_wide = pd.read_parquet(VOLUME_PATH)\n",
    "\n",
    "# Normaliza colunas de data/index\n",
    "if 'date' in close_wide.columns:\n",
    "    close_wide = close_wide.set_index('date')\n",
    "if 'date' in volume_wide.columns:\n",
    "    volume_wide = volume_wide.set_index('date')\n",
    "\n",
    "close_wide.index = pd.to_datetime(close_wide.index)\n",
    "volume_wide.index = pd.to_datetime(volume_wide.index)\n",
    "\n",
    "# Máscara de datas B3: interseção dos índices e remoção de finais de semana\n",
    "b3_idx = close_wide.index.intersection(volume_wide.index)\n",
    "b3_idx = pd.DatetimeIndex(b3_idx)\n",
    "b3_idx = b3_idx[b3_idx.dayofweek < 5]\n",
    "\n",
    "close_wide = close_wide.loc[b3_idx].sort_index()\n",
    "volume_wide = volume_wide.loc[b3_idx].sort_index()\n",
    "\n",
    "# Long format\n",
    "close_long = close_wide.stack().rename('close').to_frame().reset_index()\n",
    "close_long.columns = ['date','ticker','close']\n",
    "\n",
    "volume_long = volume_wide.stack().rename('volume').to_frame().reset_index()\n",
    "volume_long.columns = ['date','ticker','volume']\n",
    "\n",
    "panel = close_long.merge(volume_long, on=['date','ticker'], how='inner').sort_values(['ticker','date']).reset_index(drop=True)\n",
    "\n",
    "# Função de pct_change segura a divisão por zero e tipos\n",
    "\n",
    "def pct_change_safe(s: pd.Series, periods: int = 1) -> pd.Series:\n",
    "    s = pd.to_numeric(s, errors='coerce').astype(float)\n",
    "    prev = s.shift(periods)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        out = s / prev - 1.0\n",
    "    # quando prev==0, define NaN para evitar ZeroDivisionError\n",
    "    out = out.mask(prev == 0, np.nan)\n",
    "    return out\n",
    "\n",
    "# Features por ticker (passado)\n",
    "\n",
    "def per_ticker_features(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = g.sort_values('date').copy()\n",
    "    g['close'] = pd.to_numeric(g['close'], errors='coerce').astype(float)\n",
    "    g['volume'] = pd.to_numeric(g['volume'], errors='coerce').astype(float)\n",
    "\n",
    "    # retornos (seguros)\n",
    "    g['ret1'] = pct_change_safe(g['close'], 1)\n",
    "    g['ret2'] = pct_change_safe(g['close'], 2)\n",
    "    g['ret3'] = pct_change_safe(g['close'], 3)\n",
    "    g['ret5'] = pct_change_safe(g['close'], 5)\n",
    "\n",
    "    # z-scores (janela 252d)\n",
    "    for k in [1,2,3,5]:\n",
    "        r = g[f'ret{k}']\n",
    "        mu = r.rolling(252, min_periods=60).mean()\n",
    "        sd = r.rolling(252, min_periods=60).std()\n",
    "        g[f'z{k}'] = (r - mu) / sd.replace(0,np.nan)\n",
    "\n",
    "    # vol 21d\n",
    "    g['vol21'] = g['ret1'].rolling(21, min_periods=10).std()\n",
    "\n",
    "    # percentil da vol21 rolling 252\n",
    "    def last_pct(x):\n",
    "        s = pd.Series(x)\n",
    "        return (s.rank(pct=True).iloc[-1] if len(s) else np.nan)\n",
    "    g['vol21_pct'] = g['vol21'].rolling(252, min_periods=60).apply(last_pct, raw=False)\n",
    "\n",
    "    # RVOL percentil (volume) e variação diária\n",
    "    v = g['volume']\n",
    "    g['rvol_pct'] = v.rolling(252, min_periods=60).apply(last_pct, raw=False)\n",
    "    g['rvol_chg'] = g['rvol_pct'].diff()\n",
    "\n",
    "    # CLV precisa high/low (não temos nos silver informados). Deixamos NaN para manter esquema.\n",
    "    g['clv'] = np.nan\n",
    "\n",
    "    # distância ao pico 20d em σ (usando std21 e close)\n",
    "    roll_max20 = g['close'].rolling(20, min_periods=10).max()\n",
    "    std21 = g['vol21']\n",
    "    denom = (std21 * g['close']).replace(0, np.nan)\n",
    "    g['dist_peak20_sigma'] = (roll_max20 - g['close']) / denom\n",
    "\n",
    "    return g\n",
    "\n",
    "feat = panel.groupby('ticker', group_keys=False).apply(per_ticker_features)\n",
    "\n",
    "# Cross-section sinais por dia (usa z2 real)\n",
    "agg = feat.groupby('date').agg(\n",
    "    pct_z2_le_m1 = ('z2', lambda s: float(np.mean(s <= -1.0))),\n",
    "    med_vol21    = ('vol21', 'median'),\n",
    ").reset_index()\n",
    "\n",
    "feat = feat.merge(agg, on='date', how='left')\n",
    "\n",
    "# Ordena e salva\n",
    "feat = feat.sort_values(['ticker','date']).reset_index(drop=True)\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "parq_path = OUT_DIR / 'gold_rl_tabular.parquet'\n",
    "csv_path  = OUT_DIR / 'gold_rl_tabular.csv'\n",
    "\n",
    "feat.to_parquet(parq_path, index=False)\n",
    "feat.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Salvo: {parq_path} e {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ac099",
   "metadata": {},
   "source": [
    "## Fase 1 — Advantage regression (GBM) com compra por seleção e punição assimétrica\n",
    "\n",
    "Implementa o alvo A_t = (1+α)D_t - cost + U*_t - φ·max(0, δ - D_t),\n",
    "com compra determinística do melhor papel (U*_t = max_j U_t(j)), quarentena Q=5 na simulação,\n",
    "e varredura de limiar por bucket de liquidez (tercis de ADTV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "577684b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_40964\\355856000.py:192: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  val_res = val.groupby('liq_bucket', dropna=False).apply(sweep).reset_index(level=0).rename(columns={'level_0':'liq_bucket'})\n",
      "C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_40964\\355856000.py:192: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_res = val.groupby('liq_bucket', dropna=False).apply(sweep).reset_index(level=0).rename(columns={'level_0':'liq_bucket'})\n",
      "C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_40964\\355856000.py:193: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  print(\"Sweep/Val head:\\n\", val_res.groupby('liq_bucket').head(3))\n",
      "C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_40964\\355856000.py:202: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  tau_by_bucket = val_res.groupby('liq_bucket').apply(pick_tau)\n",
      "C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_40964\\355856000.py:202: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  tau_by_bucket = val_res.groupby('liq_bucket').apply(pick_tau)\n",
      "C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_40964\\355856000.py:209: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for b, grp in dfp.groupby('liq_bucket'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweep/Val head:\n",
      "   liq_bucket  thr  recall_big  precision  turnover     net_PnL\n",
      "0        low -0.5    0.211227   0.570694  0.211912  435.643420\n",
      "1        low -0.4    0.211227   0.570694  0.211912  435.643420\n",
      "2        low -0.3    0.211227   0.570694  0.211912  435.643420\n",
      "0        mid -0.5    0.218776   0.668064  0.216633  450.894257\n",
      "1        mid -0.4    0.218776   0.668064  0.216633  450.894257\n",
      "2        mid -0.3    0.218776   0.668064  0.216633  450.894257\n",
      "0       high -0.5    0.212556   0.643649  0.213002  442.297068\n",
      "1       high -0.4    0.212556   0.643649  0.213002  442.297068\n",
      "2       high -0.3    0.212556   0.643649  0.213002  442.297068\n",
      "τ por bucket: {'low': 0.20000000000000007, 'mid': -0.5, 'high': -0.5}\n",
      "Test per-bucket:\n",
      "   thr  recall_big  precision  turnover     net_PnL\n",
      "0  0.2    0.205024   0.566349  0.209980  352.043969\n",
      "1 -0.5    0.221307   0.655172  0.216536  383.774710\n",
      "2 -0.5    0.211654   0.636052  0.212127  369.952916\n",
      "Test aggregate: {'recall_big': 0.2126613930862462, 'precision': 0.6191908572066196, 'turnover': 0.21288094902387003, 'net_PnL': 1105.7715948505888}\n"
     ]
    }
   ],
   "source": [
    "# Advantage regression (GBM) — SELL vs HOLD com compra por seleção\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    _HAS_LGBM = True\n",
    "except Exception:\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    _HAS_LGBM = False\n",
    "\n",
    "# Hiperparâmetros iniciais\n",
    "ALPHA = 3.0\n",
    "COST  = 0.0025   # 25 bps ida+volta\n",
    "PHI   = 0.25\n",
    "Q     = 5\n",
    "W     = np.array([1.0, 0.6, 0.3])\n",
    "AHEAD = [1,2,3,4,5]\n",
    "\n",
    "csv_path = Path(r\"G:\\Drives compartilhados\\BOLSA_2026\\a_bolsa2026_gemini\\00_data\\03_final\\gold_rl_tabular.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(['ticker','date']).reset_index(drop=True)\n",
    "\n",
    "# Log-retornos à frente seguros (tratando zeros como missing)\n",
    "for k in AHEAD:\n",
    "    fwd = df.groupby('ticker')['close'].shift(-k)\n",
    "    ratio = fwd / df['close']\n",
    "    ratio = ratio.mask((df['close']<=0) | (fwd<=0))\n",
    "    df[f'logfwd{k}'] = np.log(ratio)\n",
    "\n",
    "# D_t(i) = max(0, - min_{h in H} sum_{k=1..h} r_{t+k}) usando log-returns\n",
    "cum1 = df['logfwd1']\n",
    "cum3 = df[['logfwd1','logfwd2','logfwd3']].sum(axis=1)\n",
    "cum5 = df[['logfwd1','logfwd2','logfwd3','logfwd4','logfwd5']].sum(axis=1)\n",
    "min_cum = pd.concat([cum1, cum3, cum5], axis=1).min(axis=1)\n",
    "df['D_t'] = np.maximum(0.0, -min_cum)\n",
    "\n",
    "# U_t(j) = w1*r_{t+1} + w3*sum_{1..3} + w5*sum_{1..5}\n",
    "U1 = cum1\n",
    "U3 = cum3\n",
    "U5 = cum5\n",
    "U  = W[0]*U1 + W[1]*U3 + W[2]*U5\n",
    "\n",
    "# Determina U*_t (melhor j) por data entre papéis elegíveis (aqui, todos)\n",
    "best_U_by_date = U.groupby(df['date']).max()\n",
    "df = df.join(best_U_by_date.rename('U_star'), on='date')\n",
    "\n",
    "# δ por ticker: p65(D_t>0)\n",
    "delta_by_ticker = (df[df['D_t']>0].groupby('ticker')['D_t'].quantile(0.65)).rename('delta')\n",
    "df = df.join(delta_by_ticker, on='ticker')\n",
    "# fallback se não houver amostras >0\n",
    "df['delta'] = df['delta'].fillna(df['D_t'].quantile(0.65))\n",
    "\n",
    "# Alvo advantage: A_t = (1+α)D_t - COST + U* - PHI*max(0, δ - D_t)\n",
    "df['A_t'] = (1.0 + ALPHA)*df['D_t'] - COST + df['U_star'] - PHI * np.maximum(0.0, df['delta'] - df['D_t'])\n",
    "\n",
    "# Features (somente passado)\n",
    "base_feats = ['z1','z2','z3','z5','vol21','vol21_pct','rvol_pct','rvol_chg','clv','dist_peak20_sigma','pct_z2_le_m1','med_vol21']\n",
    "feature_cols = [c for c in base_feats if c in df.columns]\n",
    "\n",
    "# Split walk-forward (70/15/15)\n",
    "all_dates = np.sort(df['date'].unique())\n",
    "tr_end = all_dates[int(0.7*len(all_dates))]\n",
    "va_end = all_dates[int(0.85*len(all_dates))]\n",
    "train = df[df['date'] <= tr_end].copy()\n",
    "val   = df[(df['date'] > tr_end) & (df['date'] <= va_end)].copy()\n",
    "test  = df[df['date'] > va_end].copy()\n",
    "\n",
    "# Drop NaNs no alvo somente no treino\n",
    "train = train.dropna(subset=['A_t'])\n",
    "\n",
    "Xtr, ytr = train[feature_cols].fillna(0.0), train['A_t']\n",
    "Xva, yva = val[feature_cols].fillna(0.0), val['A_t']\n",
    "Xte, yte = test[feature_cols].fillna(0.0), test['A_t']\n",
    "\n",
    "# Modelo GBM\n",
    "if _HAS_LGBM:\n",
    "    model = LGBMRegressor(\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=-1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='quantile',   # conservador\n",
    "        alpha=0.3,\n",
    "        random_state=42\n",
    "    )\n",
    "else:\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        loss='quantile',\n",
    "        alpha=0.3,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "model.fit(Xtr, ytr)\n",
    "val['score'] = model.predict(Xva)\n",
    "test['score'] = model.predict(Xte)\n",
    "\n",
    "# Calibração isotônica (apenas em pares válidos)\n",
    "try:\n",
    "    mask = (~val['score'].isna()) & (~yva.isna())\n",
    "    if mask.any():\n",
    "        iso = IsotonicRegression(out_of_bounds='clip')\n",
    "        iso.fit(val.loc[mask, 'score'], yva.loc[mask])\n",
    "        val.loc[mask, 'score'] = 0.9 * iso.transform(val.loc[mask, 'score'])\n",
    "        test['score'] = 0.9 * iso.transform(test['score'])\n",
    "    else:\n",
    "        val['score'] *= 0.9\n",
    "        test['score'] *= 0.9\n",
    "except Exception:\n",
    "    val['score'] *= 0.9\n",
    "    test['score'] *= 0.9\n",
    "\n",
    "# ADTV ~ média de volume 21d por ticker\n",
    "# criar coluna adtv21 já alinhada a df\n",
    "adtv21 = df.groupby('ticker')['volume'].rolling(21, min_periods=10).mean().reset_index(level=0, drop=True)\n",
    "df_with_adtv = df[['date','ticker']].copy()\n",
    "df_with_adtv['adtv21'] = adtv21.values\n",
    "\n",
    "val = val.merge(df_with_adtv, on=['date','ticker'], how='left')\n",
    "test = test.merge(df_with_adtv, on=['date','ticker'], how='left')\n",
    "\n",
    "# Buckets de liquidez\n",
    "try:\n",
    "    val['liq_bucket'] = pd.qcut(val['adtv21'], q=3, labels=['low','mid','high'])\n",
    "    test['liq_bucket'] = pd.qcut(test['adtv21'], q=3, labels=['low','mid','high'])\n",
    "except Exception:\n",
    "    val['liq_bucket'] = 'all'\n",
    "    test['liq_bucket'] = 'all'\n",
    "\n",
    "# Simulador com quarentena e compra por seleção (usa U_star já calculado por data)\n",
    "THRS = np.linspace(-0.5, 1.5, 21)  # varredura mais ampla no score\n",
    "\n",
    "\n",
    "def simulate_advantage(dfp: pd.DataFrame, thr: float) -> dict:\n",
    "    dfp = dfp.sort_values(['ticker','date']).copy()\n",
    "    dfp['action'] = 'HOLD'\n",
    "    dfp['cost'] = 0.0\n",
    "    dfp['avoid'] = 0.0\n",
    "    dfp['uplift'] = 0.0\n",
    "    for tk, g in dfp.groupby('ticker'):\n",
    "        q_until = pd.Timestamp.min\n",
    "        for idx in g.index:\n",
    "            d = dfp.at[idx, 'date']\n",
    "            act = 'HOLD'\n",
    "            cst = 0.0\n",
    "            if d >= q_until and dfp.at[idx, 'score'] >= thr:\n",
    "                act = 'SELL'\n",
    "                cst = COST\n",
    "                q_until = d + pd.tseries.offsets.BDay(Q)\n",
    "            Dt = dfp.at[idx, 'D_t']\n",
    "            Ustar = dfp.at[idx, 'U_star']\n",
    "            # tratar NaNs\n",
    "            Dt = 0.0 if pd.isna(Dt) else Dt\n",
    "            Ustar = 0.0 if pd.isna(Ustar) else Ustar\n",
    "            avoid = Dt if (act=='SELL') else 0.0\n",
    "            uplift = (Ustar if act=='SELL' else 0.0)\n",
    "            dfp.at[idx, 'action'] = act\n",
    "            dfp.at[idx, 'cost'] = cst\n",
    "            dfp.at[idx, 'avoid'] = avoid\n",
    "            dfp.at[idx, 'uplift'] = uplift\n",
    "    sold = (dfp['action']=='SELL')\n",
    "    recall_big = float(np.mean(dfp.loc[dfp['D_t']>0, 'action']=='SELL')) if (dfp['D_t']>0).any() else 0.0\n",
    "    precision  = float(np.mean(dfp.loc[sold, 'D_t']>0)) if sold.any() else 0.0\n",
    "    total_cost = float(dfp['cost'].sum())\n",
    "    total_avoid= float(dfp['avoid'].sum())\n",
    "    total_upl  = float(dfp['uplift'].sum())\n",
    "    turnover   = float(sold.mean())\n",
    "    net_pnl    = total_avoid - total_cost + total_upl\n",
    "    return {\n",
    "        'thr': thr,\n",
    "        'recall_big': recall_big,\n",
    "        'precision': precision,\n",
    "        'turnover': turnover,\n",
    "        'net_PnL': net_pnl\n",
    "    }\n",
    "\n",
    "# Varredura por buckets\n",
    "def sweep(dfp: pd.DataFrame):\n",
    "    out = []\n",
    "    for thr in THRS:\n",
    "        out.append(simulate_advantage(dfp, thr))\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "val_res = val.groupby('liq_bucket', dropna=False).apply(sweep).reset_index(level=0).rename(columns={'level_0':'liq_bucket'})\n",
    "print(\"Sweep/Val head:\\n\", val_res.groupby('liq_bucket').head(3))\n",
    "\n",
    "# Escolha de τ por bucket maximizando net PnL com recall_big >= 0.6 e turnover em [1%,5%]\n",
    "def pick_tau(df_res: pd.DataFrame):\n",
    "    cand = df_res[(df_res['recall_big'] >= 0.6) & (df_res['turnover'].between(0.01, 0.05))]\n",
    "    if len(cand)==0:\n",
    "        cand = df_res.sort_values('net_PnL', ascending=False)\n",
    "    return cand.iloc[0]['thr'] if len(cand)>0 else np.nan\n",
    "\n",
    "tau_by_bucket = val_res.groupby('liq_bucket').apply(pick_tau)\n",
    "print(\"τ por bucket:\", tau_by_bucket.to_dict())\n",
    "\n",
    "# Avaliação em Test usando τ por bucket\n",
    "\n",
    "def eval_with_tau(dfp: pd.DataFrame, tau_map: dict):\n",
    "    rows = []\n",
    "    for b, grp in dfp.groupby('liq_bucket'):\n",
    "        tau = tau_map.get(b, np.nan)\n",
    "        if pd.isna(tau):\n",
    "            continue\n",
    "        rows.append(simulate_advantage(grp, tau))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "test_res = eval_with_tau(test, tau_by_bucket.to_dict())\n",
    "print(\"Test per-bucket:\")\n",
    "print(test_res)\n",
    "print(\"Test aggregate:\", {\n",
    "    'recall_big': float(test_res['recall_big'].mean()) if len(test_res) else 0.0,\n",
    "    'precision': float(test_res['precision'].mean()) if len(test_res) else 0.0,\n",
    "    'turnover': float(test_res['turnover'].mean()) if len(test_res) else 0.0,\n",
    "    'net_PnL': float(test_res['net_PnL'].sum()) if len(test_res) else 0.0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93bc17c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_39464\\1903640154.py:169: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_tau_by_ticker = val.groupby('ticker').apply(pick_tau_per_ticker)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "τ_i (Val) — primeiros 10:\n",
      "ticker\n",
      "ABEV3.SA   -0.5\n",
      "B3SA3.SA   -0.5\n",
      "BBAS3.SA   -0.5\n",
      "CPLE6.SA   -0.5\n",
      "CSNA3.SA   -0.5\n",
      "ELET3.SA   -0.5\n",
      "GGBR4.SA   -0.5\n",
      "HAPV3.SA   -0.5\n",
      "ITUB4.SA   -0.5\n",
      "LREN3.SA   -0.5\n",
      "dtype: float64\n",
      "Test per-ticker — primeiros 10:\n",
      "   thr  recall_big  precision  turnover    net_PnL    ticker  tau\n",
      "0 -0.5    0.202279   0.663551  0.208577  34.231615  ABEV3.SA -0.5\n",
      "1 -0.5    0.193642   0.626168  0.208171  35.635915  B3SA3.SA -0.5\n",
      "2 -0.5    0.206061   0.635514  0.208171  33.905012  BBAS3.SA -0.5\n",
      "3 -0.5    0.181529   0.532710  0.208984  32.540322  CPLE6.SA -0.5\n",
      "4 -0.5    0.202817   0.672897  0.208577  38.934704  CSNA3.SA -0.5\n",
      "5 -0.5    0.185759   0.560748  0.208577  33.362116  ELET3.SA -0.5\n",
      "6 -0.5    0.222222   0.710280  0.208577  35.642827  GGBR4.SA -0.5\n",
      "7 -0.5    0.191549   0.635514  0.208577  38.712888  HAPV3.SA -0.5\n",
      "8 -0.5    0.201923   0.588785  0.208171  33.022387  ITUB4.SA -0.5\n",
      "9 -0.5    0.214925   0.672897  0.208171  36.723370  LREN3.SA -0.5\n",
      "Test aggregate (per-ticker τ): {'recall_big': 0.19550796327371578, 'precision': 0.6068554010706524, 'turnover': 0.20575515796012817, 'net_PnL': 1091.5969818035267}\n",
      "Arquivos salvos em: G:\\Drives compartilhados\\BOLSA_2026\\a_bolsa2026_gemini\\04_outputs\n"
     ]
    }
   ],
   "source": [
    "# Otimização por ticker: τ_i por validação e avaliação em teste (autossuficiente)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Constantes (cai de pé se não vierem da célula 6)\n",
    "try:\n",
    "    ALPHA\n",
    "except NameError:\n",
    "    ALPHA = 3.0\n",
    "try:\n",
    "    COST\n",
    "except NameError:\n",
    "    COST = 0.0025  # 25 bps\n",
    "try:\n",
    "    PHI\n",
    "except NameError:\n",
    "    PHI = 0.25\n",
    "try:\n",
    "    Q\n",
    "except NameError:\n",
    "    Q = 5\n",
    "try:\n",
    "    W\n",
    "except NameError:\n",
    "    W = np.array([1.0, 0.6, 0.3])\n",
    "AHEAD = [1,2,3,4,5]\n",
    "\n",
    "csv_path = Path(r\"G:\\Drives compartilhados\\BOLSA_2026\\a_bolsa2026_gemini\\00_data\\03_final\\gold_rl_tabular.csv\")\n",
    "\n",
    "# Requisitos para per-ticker: val/test com colunas ['date','ticker','D_t','U_star','score']\n",
    "\n",
    "def have_val_test_env():\n",
    "    return ('val' in globals()) and ('test' in globals()) and all(\n",
    "        all(c in globals()[name].columns for c in ['date','ticker','D_t','U_star','score'])\n",
    "        for name in ['val','test']\n",
    "    )\n",
    "\n",
    "# Se não existir 'val' e 'test' prontos, reconstrói mínimos + modelo rápido para obter 'score'\n",
    "if not have_val_test_env():\n",
    "    try:\n",
    "        from lightgbm import LGBMRegressor\n",
    "        _HAS_LGBM = True\n",
    "    except Exception:\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        _HAS_LGBM = False\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values(['ticker','date']).reset_index(drop=True)\n",
    "    # logfwd seguros\n",
    "    for k in AHEAD:\n",
    "        fwd = df.groupby('ticker')['close'].shift(-k)\n",
    "        ratio = (fwd / df['close']).mask((df['close']<=0) | (fwd<=0))\n",
    "        df[f'logfwd{k}'] = np.log(ratio)\n",
    "    cum1 = df['logfwd1']\n",
    "    cum3 = df[['logfwd1','logfwd2','logfwd3']].sum(axis=1)\n",
    "    cum5 = df[['logfwd1','logfwd2','logfwd3','logfwd4','logfwd5']].sum(axis=1)\n",
    "    min_cum = pd.concat([cum1, cum3, cum5], axis=1).min(axis=1)\n",
    "    df['D_t'] = np.maximum(0.0, -min_cum)\n",
    "    U = W[0]*cum1 + W[1]*cum3 + W[2]*cum5\n",
    "    best_U_by_date = U.groupby(df['date']).max()\n",
    "    df = df.join(best_U_by_date.rename('U_star'), on='date')\n",
    "    # δ por ticker\n",
    "    delta_by_ticker = (df[df['D_t']>0].groupby('ticker')['D_t'].quantile(0.65)).rename('delta')\n",
    "    df = df.join(delta_by_ticker, on='ticker')\n",
    "    df['delta'] = df['delta'].fillna(df['D_t'].quantile(0.65))\n",
    "    # A_t\n",
    "    df['A_t'] = (1.0 + ALPHA)*df['D_t'] - COST + df['U_star'] - PHI * np.maximum(0.0, df['delta'] - df['D_t'])\n",
    "    # features\n",
    "    base_feats = ['z1','z2','z3','z5','vol21','vol21_pct','rvol_pct','rvol_chg','clv','dist_peak20_sigma','pct_z2_le_m1','med_vol21']\n",
    "    feature_cols = [c for c in base_feats if c in df.columns]\n",
    "    # split\n",
    "    all_dates = np.sort(df['date'].unique())\n",
    "    tr_end = all_dates[int(0.7*len(all_dates))]\n",
    "    va_end = all_dates[int(0.85*len(all_dates))]\n",
    "    train = df[df['date'] <= tr_end].copy()\n",
    "    val   = df[(df['date'] > tr_end) & (df['date'] <= va_end)].copy()\n",
    "    test  = df[df['date'] > va_end].copy()\n",
    "    # train model\n",
    "    train = train.dropna(subset=['A_t'])\n",
    "    Xtr, ytr = train[feature_cols].fillna(0.0), train['A_t']\n",
    "    Xva = val[feature_cols].fillna(0.0)\n",
    "    Xte = test[feature_cols].fillna(0.0)\n",
    "    if _HAS_LGBM:\n",
    "        model = LGBMRegressor(\n",
    "            n_estimators=400,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=-1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            objective='quantile',\n",
    "            alpha=0.3,\n",
    "            random_state=42\n",
    "        )\n",
    "    else:\n",
    "        model = GradientBoostingRegressor(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=3,\n",
    "            loss='quantile',\n",
    "            alpha=0.3,\n",
    "            random_state=42\n",
    "        )\n",
    "    model.fit(Xtr, ytr)\n",
    "    val['score'] = model.predict(Xva)\n",
    "    test['score'] = model.predict(Xte)\n",
    "\n",
    "# Parâmetros da varredura e critérios\n",
    "THRS = np.linspace(-0.5, 1.5, 41)\n",
    "TARGET_RECALL = 0.6\n",
    "TURNOVER_MIN, TURNOVER_MAX = 0.01, 0.05\n",
    "\n",
    "\n",
    "def simulate_advantage(dfp: pd.DataFrame, thr: float) -> dict:\n",
    "    dfp = dfp.sort_values(['ticker','date']).copy()\n",
    "    dfp['action'] = 'HOLD'\n",
    "    dfp['cost'] = 0.0\n",
    "    dfp['avoid'] = 0.0\n",
    "    dfp['uplift'] = 0.0\n",
    "    for tk, g in dfp.groupby('ticker'):\n",
    "        q_until = pd.Timestamp.min\n",
    "        for idx in g.index:\n",
    "            d = dfp.at[idx, 'date']\n",
    "            act = 'HOLD'\n",
    "            cst = 0.0\n",
    "            if d >= q_until and dfp.at[idx, 'score'] >= thr:\n",
    "                act = 'SELL'\n",
    "                cst = COST\n",
    "                q_until = d + pd.tseries.offsets.BDay(Q)\n",
    "            Dt = dfp.at[idx, 'D_t']\n",
    "            Ustar = dfp.at[idx, 'U_star']\n",
    "            Dt = 0.0 if pd.isna(Dt) else Dt\n",
    "            Ustar = 0.0 if pd.isna(Ustar) else Ustar\n",
    "            avoid = Dt if (act=='SELL') else 0.0\n",
    "            uplift = (Ustar if act=='SELL' else 0.0)\n",
    "            dfp.at[idx, 'action'] = act\n",
    "            dfp.at[idx, 'cost'] = cst\n",
    "            dfp.at[idx, 'avoid'] = avoid\n",
    "            dfp.at[idx, 'uplift'] = uplift\n",
    "    sold = (dfp['action']=='SELL')\n",
    "    recall_big = float(np.mean(dfp.loc[dfp['D_t']>0, 'action']=='SELL')) if (dfp['D_t']>0).any() else 0.0\n",
    "    precision  = float(np.mean(dfp.loc[sold, 'D_t']>0)) if sold.any() else 0.0\n",
    "    total_cost = float(dfp['cost'].sum())\n",
    "    total_avoid= float(dfp['avoid'].sum())\n",
    "    total_upl  = float(dfp['uplift'].sum())\n",
    "    turnover   = float(sold.mean())\n",
    "    net_pnl    = total_avoid - total_cost + total_upl\n",
    "    return {\n",
    "        'thr': thr,\n",
    "        'recall_big': recall_big,\n",
    "        'precision': precision,\n",
    "        'turnover': turnover,\n",
    "        'net_PnL': net_pnl\n",
    "    }\n",
    "\n",
    "# Escolha de τ por ticker\n",
    "\n",
    "def pick_tau_per_ticker(g: pd.DataFrame) -> float:\n",
    "    res = []\n",
    "    for thr in THRS:\n",
    "        res.append(simulate_advantage(g, thr))\n",
    "    res = pd.DataFrame(res)\n",
    "    cand = res[(res['recall_big'] >= TARGET_RECALL) & (res['turnover'].between(TURNOVER_MIN, TURNOVER_MAX))]\n",
    "    if len(cand)==0:\n",
    "        cand = res.sort_values('net_PnL', ascending=False)\n",
    "    return float(cand.iloc[0]['thr']) if len(cand)>0 else np.nan\n",
    "\n",
    "# τ_i na validação por ticker\n",
    "val_tau_by_ticker = val.groupby('ticker').apply(pick_tau_per_ticker)\n",
    "print(\"τ_i (Val) — primeiros 10:\")\n",
    "print(val_tau_by_ticker.head(10))\n",
    "\n",
    "# Avaliação em Teste com τ_i\n",
    "rows = []\n",
    "for tk, g in test.groupby('ticker'):\n",
    "    tau_i = val_tau_by_ticker.get(tk, np.nan)\n",
    "    if pd.isna(tau_i) or len(g)==0:\n",
    "        continue\n",
    "    rows.append({**simulate_advantage(g, tau_i), 'ticker': tk, 'tau': tau_i})\n",
    "\n",
    "test_per_ticker = pd.DataFrame(rows)\n",
    "print(\"Test per-ticker — primeiros 10:\")\n",
    "print(test_per_ticker.head(10))\n",
    "\n",
    "# Agregados e salvamento\n",
    "agg = {\n",
    "    'recall_big': float(test_per_ticker['recall_big'].mean()) if len(test_per_ticker) else 0.0,\n",
    "    'precision': float(test_per_ticker['precision'].mean()) if len(test_per_ticker) else 0.0,\n",
    "    'turnover': float(test_per_ticker['turnover'].mean()) if len(test_per_ticker) else 0.0,\n",
    "    'net_PnL': float(test_per_ticker['net_PnL'].sum()) if len(test_per_ticker) else 0.0\n",
    "}\n",
    "print(\"Test aggregate (per-ticker τ):\", agg)\n",
    "\n",
    "# Salvar resultados\n",
    "out_dir = Path(r\"G:\\Drives compartilhados\\BOLSA_2026\\a_bolsa2026_gemini\\04_outputs\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "val_tau_by_ticker.to_csv(out_dir / 'val_tau_by_ticker.csv')\n",
    "test_per_ticker.to_csv(out_dir / 'test_per_ticker_metrics.csv', index=False)\n",
    "print(\"Arquivos salvos em:\", out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
